{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f89dc0",
   "metadata": {},
   "source": [
    "# Notebook to train the Hybrid ResNet-ViT & ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31691fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diogommiranda/tese/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-10 13:04:00.112418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-10 13:04:00.852795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Automatic Mixed Precision (AMP) enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import GradScaler\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, matthews_corrcoef, roc_auc_score, accuracy_score, recall_score\n",
    "from data_utils_torch import get_paths_and_labels, calculate_min_max, create_dataloader, extract_subject_id, clean_zone_identifier_files\n",
    "from plotting_utils_torch import view_image, view_random_image, plot_loss_curves, make_confusion_matrix, view_image_data, plot_guidance_losses, plot_average_attention_scores\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from models3D import ResnetFeatureExtractor, ResNetViT, pureViT, upsample_vit_attention\n",
    "import json\n",
    "import nibabel as nib\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility    \n",
    "seed = 10\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Set mixed precision     \n",
    "USE_AMP = True if device.type == 'cuda' else False\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "print(f\"Automatic Mixed Precision (AMP) enabled: {USE_AMP}\")\n",
    "\n",
    "#clean_zone_identifier_files(\"/home/diogommiranda/tese/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72529326",
   "metadata": {},
   "source": [
    "# Select which Model to use\n",
    "Note:\n",
    "- When using ResNet and Hybrid Models the input shape is (91, 109, 91)\n",
    "- When using PureViT the input shape is padded to (96, 112, 96) to make it divisible by a patch size of (16,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1fca9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hybrid model. \n",
      "Expected input shape is (91, 109, 91)\n"
     ]
    }
   ],
   "source": [
    "USE_MODEL = \"hybrid\" # Choose model: \"hybrid\", \"purevit\"\n",
    "\n",
    "if USE_MODEL == \"hybrid\":\n",
    "    print(\"Using Hybrid model. \\nExpected input shape is (91, 109, 91)\")\n",
    "    apply_padding = False\n",
    "    EXPECTED_FINAL_VOLUME_SHAPE = (91, 109, 91)\n",
    "    \n",
    "elif USE_MODEL == \"purevit\":\n",
    "    print(\"Using pure ViT model. \\nExpected input shape is (96, 112, 96)\")\n",
    "    apply_padding = True\n",
    "    EXPECTED_FINAL_VOLUME_SHAPE = (96, 112, 96)\n",
    "else:\n",
    "    raise ValueError(\"Invalid model choice. Use 'hybrid' or 'purevit'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180a60dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class map: {'smci': 0, 'pmci': 1}\n",
      "Scanning /home/diogommiranda/tese/datasets/mni_reg_CV/smci_pmci_balanced/train...\n",
      "    Found 349 files for class 'smci'\n",
      "    Found 314 files for class 'pmci'\n",
      "Calculated Min: -0.3496308922767639, Max: 2.7912356853485107\n",
      "Class map: {'smci': 0, 'pmci': 1}\n",
      "Scanning /home/diogommiranda/tese/datasets/mni_reg_CV/smci_pmci_balanced/test...\n",
      "    Found 87 files for class 'smci'\n",
      "    Found 78 files for class 'pmci'\n"
     ]
    }
   ],
   "source": [
    "NORMALIZATION = \"mni_reg_CV\" # Name of the directory with the datasets\n",
    "DATASET = \"smci_pmci_balanced\" # Choose dataset: \"smci_pmci_balanced\", \"smci_pmci\" or \"nc_ad\"\n",
    "DATA_DIR = Path(\"/home/diogommiranda/tese/datasets/\") / NORMALIZATION / DATASET\n",
    "VOLUME_SHAPE = (91, 109, 91) # Expected volume shape of the PET images\n",
    "BATCH_SIZE = 4 # Set batch size\n",
    "\n",
    "# Get train paths and labels to calculate minmax values\n",
    "train_paths, train_labels, class_map = get_paths_and_labels(DATA_DIR, 'train')\n",
    "train_paths = np.array(train_paths)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Calculate minmax values for train set\n",
    "minmax_min, minmax_max = calculate_min_max(train_paths)\n",
    "\n",
    "# Create train dataset\n",
    "train_data = create_dataloader(\n",
    "    paths=train_paths,\n",
    "    labels=train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    volume_shape=VOLUME_SHAPE,\n",
    "    is_training=True,\n",
    "    seed=seed,\n",
    "    min_val=minmax_min, \n",
    "    max_val=minmax_max,\n",
    "    apply_padding=apply_padding\n",
    "    )\n",
    "\n",
    "# Create test set with the minmax values from train\n",
    "test_paths, test_labels, _ = get_paths_and_labels(DATA_DIR, 'test', class_map=class_map)\n",
    "test_data = create_dataloader(\n",
    "    paths=test_paths,\n",
    "    labels=test_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    volume_shape=VOLUME_SHAPE,\n",
    "    is_training=False,\n",
    "    seed=None,\n",
    "    min_val=minmax_min, \n",
    "    max_val=minmax_max,\n",
    "    apply_padding=apply_padding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59168ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 91, 109, 91])\n",
      "Min: 0.11078770458698273, Max: 0.8640980124473572\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify data with a test sample\n",
    "i = 0\n",
    "for data, label in test_data:\n",
    "    view_image_data(data[i])\n",
    "    #data[i] = torch.flip(data[i], dims=[3])\n",
    "    #view_image_data(data[i])\n",
    "    print(data[i].shape)\n",
    "    min_val = data[i].min().item()\n",
    "    max_val = data[i].max().item()\n",
    "    print(f\"Min: {min_val}, Max: {max_val}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add2b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting configurations for the best hybrid and pure ViT models\n",
    "\n",
    "resnet_config = {\n",
    "    'block': 'basic',\n",
    "    'layers': [2, 2, 2, 2], # [2, 2, 2, 2] for ResNet18, [3, 4, 6, 3] for ResNet34\n",
    "    'block_inplanes': [64, 128, 256, 512],\n",
    "    'spatial_dims': 3,\n",
    "    'n_input_channels': 1,\n",
    "    'conv1_t_stride': 2,\n",
    "    'num_classes': 1, \n",
    "    'shortcut_type': 'B',\n",
    "    'bias_downsample': True\n",
    "}\n",
    "\n",
    "vit_config = {\n",
    "    'in_channels': 256,\n",
    "    'img_size': (6,7,6),\n",
    "    'patch_size': (1,1,1),  \n",
    "    'num_heads': 8,\n",
    "    'hidden_size': 504,\n",
    "    'mlp_dim': 2016,\n",
    "    'num_layers': 7,\n",
    "    'proj_type': 'perceptron',\n",
    "    'pos_embed_type': 'sincos',\n",
    "    'classification': True,\n",
    "    'num_classes': 1,\n",
    "    'dropout_rate': 0.0,\n",
    "    'spatial_dims': 3,\n",
    "    'post_activation': 'none',\n",
    "    'qkv_bias': False,\n",
    "    'save_attn': True,\n",
    "    'save_attn_logits': True\n",
    "}\n",
    "\n",
    "purevit_config = {\n",
    "    'in_channels': 1,\n",
    "    'img_size': (96,112,96),\n",
    "    'patch_size': (16,16,16),\n",
    "    'num_heads': 12,\n",
    "    'hidden_size': 768,\n",
    "    'mlp_dim': 3072,\n",
    "    'num_layers': 12,\n",
    "    'proj_type': 'perceptron',\n",
    "    'pos_embed_type': 'sincos',\n",
    "    'classification': True,\n",
    "    'num_classes': 1,\n",
    "    'dropout_rate': 0.0,\n",
    "    'spatial_dims': 3,\n",
    "    'post_activation': 'none',\n",
    "    'qkv_bias': False,\n",
    "    'save_attn': True,\n",
    "    'save_attn_logits': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08449d",
   "metadata": {},
   "source": [
    "# Get ROI mask and setup variables for the guidance loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f13169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask volume shape: (1, 91, 109, 91)\n",
      "Number of ROI voxels: 37384\n",
      "Number of non-ROI voxels: 865245\n",
      "Mask volume shape after adding batch dimension: torch.Size([1, 91, 109, 91])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZwAAAHqCAYAAACA44tnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPa5JREFUeJzt3Xu8l2O+P/73Sq3OqwNySFbJzE7s2TmEKWkcViERm5Q9KF8yzE6MQ5ro5BDGobYZDEY1TY4j1DZJlD0G22lomPYmSW0MM4aIirSu3x9+rbFaq1rVpVZrPZ+Px/3Huu/rc9/X57Tuz+f1ue73VZBSSgEAAAAAAJuozpbuAAAAAAAANYPAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAADbAqFGjoqCgINv+3n777SgoKIiJEyd+a8fYEPfdd1+0bNkyPvvssy1y/BxWrlwZbdq0iZtvvnlLdwWASlR27quqiRMnRkFBQbz99tvZ+lNQUBCjRo36Vo9RVc8//3wUFhbGokWLNvuxc7vkkkvigAMO2NLdYAsQOAMAkM2CBQvirLPOit122y0aNGgQRUVF0bVr1xg/fnwsX758S3dvs5s+fXp07949WrVqFY0aNYrddtst+vbtG48++uiW7lqlVq1aFSNHjozBgwdHkyZN4tlnn406derEsGHDKm1/zTXXREFBQTzyyCObdNwf/OAHUVBQUGE54ogj1nm7K6+8MgoKCmKvvfYqt75evXrxk5/8JK688spYsWLFJvUNgH+4+eabo6CgYKsJEf/whz/EkUceGa1bt44GDRrErrvuGr1794677rprS3dtrYYPHx79+/eP4uLiiPhH+L2+JXc4fuaZZ0ZBQUEcffTRFba1bdu20j786Ec/KtfuvPPOi7lz58a0adOy9o3qr+6W7gAAADXDI488EieeeGLUr18/Tj311Nhrr73iyy+/jD/84Q9x0UUXxZ///Oe47bbbtnQ3N5vrrrsuLrrooujevXsMGzYsGjVqFG+++WY8/vjjcc8996wzTL300kvjkksu2Yy9/dr06dPj9ddfj0GDBkVExPe///0466yz4vrrr48f/vCHseeee5a1XbRoUYwZMyZOPPHE6NWr1yYfe5dddomxY8eWW7fzzjuvtf0777wTV111VTRu3LjS7QMHDoxLLrkk7rrrrjj99NM3uX8AREyZMiXatm0bzz//fLz55pux++67b9R+iouLY/ny5VGvXr3MPfyH+++/P0466aTo1KlTDBkyJFq0aBELFy6M3//+93H77bfHySefvNbbnnLKKdGvX7+oX7/+t9a/yrzyyivx+OOPxzPPPFO27uCDD47JkydX2v7dd9+NYcOGRdu2baNVq1bZ+vHiiy/GxIkTo0GDBmtt06lTp7jgggvKrfvud79b7u8dd9wxjj322LjuuuvimGOOydY/qj+BMwAAm2zhwoXRr1+/KC4ujtmzZ8dOO+1Utu3HP/5xvPnmm5s8CjYiIqUUK1asiIYNG27yvr5NX331VVx++eVRUlISjz32WIXtf/3rX9d5+7p160bdupv/o/qECROia9eu0bp167J1V199dTz88MNx1llnxVNPPVVW6mPw4MFRr169GD9+fJZjN2vWLH74wx9Wuf2FF14YBx54YKxatSo+/PDDCtubN28ePXr0iIkTJwqcATJYuHBhPPPMMzF16tQ466yzYsqUKTFy5MiN2ldBQcE6w8wcRo0aFR07doz//u//jsLCwnLb1nce3mabbWKbbbb5NrtXqQkTJsSuu+4aBx54YNm63XbbLXbbbbcKbVetWhWHHnpo1K1bN+6+++5o1KhRlj6klOLcc8+NU089NZ544om1tmvdunWVztt9+/aNE088Md56661K7wc1k5IaAABssmuvvTY+++yz+NWvflUubF5t9913jyFDhpT9vTqQbd++fdSvXz/atm0bP/3pT+OLL74od7u2bdvG0UcfHTNnzoz99tsvGjZsGL/85S8jIuKtt96KE088MVq2bBmNGjWKAw88sEKo/eSTT0ZBQUHcd999ceWVV8Yuu+wSDRo0iMMOOyzefPPNcm2feuqpOPHEE2PXXXeN+vXrR5s2beL888/fqFIgH374YXz66afRtWvXSrevbxTS2mo4/+Y3v4n9998/GjVqFC1atIiDDz64QqA9Y8aM6NatWzRu3DiaNm0avXr1ij//+c/r7fOKFSvi0UcfjcMPP7zc+mbNmsX48ePj6aefjjvuuCMiIh588MGYPn16XH311ZU+3xvrq6++qlLt6N///vfx29/+NsaNG7fOdiUlJfGHP/whPvroo0w9BKi9pkyZEi1atIhevXrFCSecEFOmTKnQZuTIkVGnTp0KQeWgQYOisLAw5s6dGxGV13D+05/+FAMGDCgry7XjjjvG6aefHn//+983qr8LFiyIzp07VwibI9Z/Hl5bDecZM2ZE9+7do2nTplFUVBSdO3euUJ7jueeeiyOOOCKaNWsWjRo1iu7du8fTTz9dpT4/9NBDceihh1ZpHofRo0fH73//+7jiiiuyljiZPHlyvPbaa3HllVeut+2XX34Zn3/++TrbrP5c8fDDD2fpH1sHgTMAAJts+vTpsdtuu0WXLl2q1P6MM86IESNGxD777BM33nhjdO/ePcaOHRv9+vWr0Pb111+P/v37R0lJSYwfPz46deoUH3zwQXTp0iVmzpwZ55xzTlmt3mOOOSYefPDBCvu4+uqr48EHH4wLL7wwhg0bFv/93/8d//Zv/1auzf333x/Lli2Ls88+O2666abo2bNn3HTTTXHqqadu8OPRqlWraNiwYUyfPj1b2Dl69Og45ZRTol69ejFmzJgYPXp0tGnTJmbPnl3WZvLkydGrV69o0qRJXHPNNXHZZZfFvHnz4qCDDlpvbceXXnopvvzyy9hnn30qbFtdNmPo0KHx1ltvxZAhQ6JLly5x1llnlWv32WefxYcffrje5ZNPPqlwjDfeeKMsJN9xxx3jsssui5UrV1Zot2rVqhg8eHCcccYZ8c///M/rvE/77rtvpJTKXZoMwMaZMmVKHH/88VFYWBj9+/eP+fPnxwsvvFCuzaWXXhqdOnWK//f//l8sXbo0IiJmzpwZt99+e4wYMSL+5V/+Za37nzVrVrz11lsxcODAuOmmm6Jfv35xzz33xFFHHRUppQ3ub3FxcTzxxBPxzjvvbPBtKzNx4sTo1atXfPTRRzFs2LC4+uqro1OnTuXmZZg9e3YcfPDB8emnn8bIkSPjqquuiiVLlsShhx4azz///Dr3/+6778bixYsrPQ+vafbs2XHllVdGz54946KLLiq3beXKlVU6F3/44YdRWlpa7rZLly6NoUOHxk9/+tPYcccd19uHRo0aRZMmTaJt27ZrveKpWbNm0b59+yqH7tQQCQAANsEnn3ySIiIde+yxVWr/yiuvpIhIZ5xxRrn1F154YYqINHv27LJ1xcXFKSLSo48+Wq7teeedlyIiPfXUU2Xrli5dmtq1a5fatm2bVq1alVJKac6cOSki0h577JG++OKLsrbjx49PEZFeffXVsnXLli2r0NexY8emgoKCtGjRorJ1I0eOTFX5GD1ixIgUEalx48bpyCOPTFdeeWV66aWXKrRbuHBhiog0YcKEtR5j/vz5qU6dOum4444ru2+rlZaWlt3/5s2bpzPPPLPc9vfffz81a9aswvo13XHHHRUek296++23U+PGjVPLli1TvXr1Km132mmnpYhY79K9e/dytzv99NPTqFGj0gMPPJB+/etfp2OOOSZFROrbt2+FY/z85z9PzZo1S3/9619TSil179497bnnnpX2+b333ksRka655pp13ncA1u3FF19MEZFmzZqVUvr63LPLLrukIUOGVGj76quvpsLCwnTGGWekjz/+OLVu3Trtt99+aeXKlWVtKjv3VXYevvvuu1NEpN///vdl6yZMmJAiIi1cuHCdff7Vr36VIiIVFhamQw45JF122WXpqaeeqnAeTSmliEgjR45c6zGWLFmSmjZtmg444IC0fPnycrddfR4uLS1N3/nOd1LPnj3L1q2+X+3atUslJSXr7O/jjz+eIiJNnz59ne0++OCDtNNOO6Udd9wxffDBBxW2r/7sU5VlzcfwwgsvTO3atUsrVqxIKX39OaxXr14VjtG7d+90zTXXpIceeij96le/St26dUsRkS6++OJK+9yjR4+0xx57rPN+UbOo4QwAwCb59NNPIyKiadOmVWr/u9/9LiIifvKTn5Rbf8EFF8R1110XjzzySBxyyCFl69u1axc9e/assI/9998/DjrooLJ1TZo0iUGDBsWwYcNi3rx5sddee5VtGzhwYLlLart16xYRX5flWN3um3WhP//881i+fHl06dIlUkrx8ssvx6677lql+7fa6NGjo0OHDnHzzTfHzJkzY8aMGTF8+PDYe++9Y8qUKbHHHntUeV8PPfRQlJaWxogRI6JOnfIXKa6+7HbWrFmxZMmS6N+/f7maxttss00ccMABMWfOnHUeY/Ulyy1atKh0e3FxcYwcOTIuvvjiGDp0aLnHd7WLL764SvUc1zzGr371q3J/n3LKKTFo0KC4/fbb4/zzzy+rZfn3v/89RowYEZdddllsv/32VT5OZTWeAai6KVOmxA477FB2fi4oKIiTTjopfvOb38T1119frt7xXnvtFaNHj45hw4bFn/70p/jwww/jscceW+/cBN88D69YsSI+++yzsv//f/zjH8vO3VV1+umnR+vWreOGG26IOXPmxJw5c+Lyyy+P3XbbLSZPnlzlq7Iivj7HLl26NC655JIKtadXn4dfeeWVmD9/flx66aUVyoAcdthhMXny5CgtLa1wHl9tfefhiK/rK5966qnxwQcfxMyZMystDfIv//IvMWvWrCrdr2+OYn7jjTdi/Pjxcffdd693ssRp06aV+3vgwIFx5JFHxg033BCDBw+OXXbZpdz2Fi1axMsvv1ylPlEzCJwBANgkRUVFERFll86uz6JFi6JOnToVZrbfcccdo3nz5rFo0aJy69u1a1fpPiqrV7g6xF20aFG5QHTNsHj1l7mPP/64bN3ixYtjxIgRMW3atHLrI6LSEhBV0b9//+jfv398+umn8dxzz8XEiRPjrrvuit69e8drr71W5QmTFixYEHXq1ImOHTuutc38+fMjIuLQQw+tdPvq52l90jouW+7cuXNEROy3336Vbu/YseM6+7ghLrjggrj99tvj8ccfLwscLr300mjZsmUMHjy4SvtYfV+qUgsTgMqtWrUq7rnnnjjkkENi4cKFZesPOOCAuP766+OJJ56IHj16lLvNRRddFPfcc088//zzcdVVV1Xp3PDRRx/F6NGj45577qkwqd/Gnod79uwZPXv2jGXLlsVLL70U9957b9x6661x9NFHx//+7/+ut5bzagsWLIiIqPTH1tVWn4dPO+20tbb55JNP1hkoR6z7PHzNNdfEzJkzY9iwYRXmXFitRYsWa922LqvLZf3rv/7rBt+2oKAgzj///Jg5c2Y8+eSTFX58Tik5F9cyAmcAADZJUVFR7LzzzvHaa69t0O2q+sXjmyOeNtbaZppf/aVu1apVUVJSEh999FEMHTo0OnToEI0bN4533303BgwYUKHG4YYqKiqKkpKSKCkpiXr16sWkSZPiueeei+7du2/Sfr9pdR8nT55cad3F9Y0s23bbbSPi6xB+zZFJVfXJJ59UaZLFwsLCaNmy5TrbtGnTJiKirAb2/Pnz47bbbotx48bFe++9V9ZuxYoVsXLlynj77bejqKio3H5X/3Cw3XbbbfB9AeBrs2fPjr/85S9xzz33xD333FNh+5QpUyoEzm+99VZZAPvqq69W6Th9+/aNZ555Ji666KLo1KlTNGnSJEpLS+OII47Y5PNwo0aNolu3btGtW7fYbrvtYvTo0TFjxox1hsMbanUff/azn0WnTp0qbdOkSZO13v6b5+HKPPvss3HZZZdFly5dYsyYMWvdz5dfflnl+SO233772GabbWL27Nnx6KOPxtSpU8vN+fDVV1/F8uXL4+23346WLVuu88frNc/b3/Txxx87F9cyAmcAADbZ0UcfHbfddls8++yz8f3vf3+dbYuLi6O0tDTmz59frqzEBx98EEuWLIni4uL1Hq+4uDhef/31Cuv/93//t2z7hnj11VfjjTfeiEmTJpWbJLCql6RuiP322y8mTZoUf/nLX6p8m/bt20dpaWnMmzdvrV9i27dvHxFfT1i4MSObOnToEBERCxcuXO9kfGszZMiQmDRp0nrbde/ePZ588sl1tnnrrbciIspKZ7z77rtRWloa5557bpx77rkV2rdr1y6GDBkS48aNK1u3eiTehpQvAaC8KVOmRKtWreIXv/hFhW1Tp06NBx98MG699dayH4hLS0tjwIABUVRUFOedd15cddVVccIJJ8Txxx+/1mN8/PHH8cQTT8To0aNjxIgRZetXh9Y5rb5KZ0PPwxERr732WoUrtNZsU1RUtMnn4TV9/PHH0a9fv2jSpEncdddd6/wR+ZlnnilXmmxdFi5cGG3bto3FixdHRFT6HL377rvRrl27uPHGG+O8885b677WPG+veZx1TRhJzSNwBgBgk1188cUxZcqUOOOMM2L27Nmxww47lNu+YMGC+M///M8YMmRIHHXUUfHTn/40xo0bF7/85S/L2txwww0REdGrV6/1Hu+oo46KcePGlQu4P//887jtttuibdu2G1zWYfUI6G9exppSWuuM6+uzbNmymDt3bqXh+4wZMyIi4p/+6Z+qvL8+ffrE0KFDY8yYMfHb3/62XP3H1Zep9uzZM4qKiuKqq66KQw45JOrVq1duH3/729/WWfd43333jcLCwnjxxRfjmGOOqXLfvmljajh/+umnUb9+/XL1IlNKccUVV0RElNXv3muvveLBBx+ssK9LL700li5dGuPHjy/7sr/aSy+9FAUFBev9EQSAyi1fvjymTp0aJ554YpxwwgkVtu+8885x9913x7Rp0+Kkk06KiK/P588880xMmzYtevXqFU8++WScffbZcfDBB691lGtl5+GIKPcj4oZ64okn4rDDDquwfvVcEhtyHu7Ro0c0bdo0xo4dG0cccUS5klirz8P77rtvtG/fPq677ro4+eSTK4xmXt95uHXr1tGmTZt48cUXK2w7/fTTY/HixfHAAw+s90f1janhfOihh1Z6jh00aFAUFxfH8OHDy36M/uijj6JZs2blrh5buXJlXH311VFYWFgh7P7kk09iwYIFcfbZZ1epT9QMAmcAADZZ+/bt46677oqTTjop9thjjzj11FNjr732ii+//DKeeeaZuP/++2PAgAER8fUXodNOOy1uu+22WLJkSXTv3j2ef/75mDRpUvTp06dKo3IuueSSuPvuu+PII4+Mc889N1q2bBmTJk2KhQsXxgMPPLDWCXnWpkOHDtG+ffu48MIL4913342ioqJ44IEH1npZ6/osW7YsunTpEgceeGAcccQR0aZNm1iyZEk89NBD8dRTT0WfPn1i7733rvL+dt999xg+fHhcfvnl0a1btzj++OOjfv368cILL8TOO+8cY8eOjaKiorjlllvilFNOiX322Sf69esX22+/fSxevDgeeeSR6Nq1a/z85z9f6zEaNGgQPXr0iMcff3ydl+quy8bUcP7jH/9YVut69913j+XLl8eDDz4YTz/9dAwaNCj22WefiPi6LEafPn0q3H51GFHZtlmzZkXXrl3LLlMGYMNMmzYtli5dutYfIg888MDYfvvtY8qUKXHSSSfF//zP/8Rll10WAwYMiN69e0dExMSJE6NTp05xzjnnxH333VfpfoqKiuLggw+Oa6+9NlauXBmtW7eOxx57rNLRvlV17LHHRrt27aJ3797Rvn37+Pzzz+Pxxx+P6dOnR+fOncv6VxVFRUVx4403xhlnnBGdO3eOk08+OVq0aBFz586NZcuWxaRJk6JOnTpxxx13xJFHHhl77rlnDBw4MFq3bh3vvvtuzJkzJ4qKimL69Onr7fODDz5YrubxrbfeGg899FB873vfi2XLlsVvfvObSm9bUlISO+yww0bVcN51110rnRz5vPPOix122KHcOXbatGlxxRVXxAknnBDt2rWLjz76KO6666547bXX4qqrrqpQ1uvxxx+PlFIce+yxG9QntnIJAAAyeeONN9KZZ56Z2rZtmwoLC1PTpk1T165d00033ZRWrFhR1m7lypVp9OjRqV27dqlevXqpTZs2adiwYeXapJRScXFx6tWrV6XHWrBgQTrhhBNS8+bNU4MGDdL++++f/vM//7Ncmzlz5qSISPfff3+59QsXLkwRkSZMmFC2bt68eenwww9PTZo0Sdttt10688wz09y5cyu0GzlyZFrfx+iVK1em22+/PfXp0ycVFxen+vXrp0aNGqW99947/exnP0tffPHFOvuytmPceeedae+9907169dPLVq0SN27d0+zZs2qcJ979uyZmjVrlho0aJDat2+fBgwYkF588cV19jmllKZOnZoKCgrS4sWLK92+tsdzU7z11lvpxBNPTG3btk0NGjRIjRo1Svvuu2+69dZbU2lp6Xpv371797TnnntWWL9kyZJUWFiY7rjjjmx9BahtevfunRo0aJA+//zztbYZMGBAqlevXvrwww9T586d0y677JKWLFlSrs348eNTRKR77703pVT5ue+dd95Jxx13XGrevHlq1qxZOvHEE9N7772XIiKNHDmyrN2ECRNSRKSFCxeus+9333136tevX2rfvn1q2LBhatCgQerYsWMaPnx4+vTTT8u1reoxpk2blrp06ZIaNmyYioqK0v7775/uvvvucm1efvnldPzxx6dtt9021a9fPxUXF6e+ffumJ554Yp39TSmlP/7xjyki0lNPPVW27rTTTksRsd5lzpw5693/hqrsc9iLL76YevfunVq3bp0KCwtTkyZN0kEHHZTuu+++Svdx0kknpYMOOih736jeClJax/SXAABArbFq1aro2LFj9O3bNy6//PIt3Z1NMm7cuLj22mtjwYIFWSaeBIDN4bDDDoudd945Jk+evKW7ssnef//9aNeuXdxzzz1GONcyAmcAAKDMvffeG2effXYsXry4Qv3JrcXKlSujffv2cckll8Q555yzpbsDAFX23HPPRbdu3WL+/PkbPAlydXPJJZfE7Nmz4/nnn9/SXWEzEzgDAAAAAJDFhs2mAgAAAAAAayFwBgAAAAAgC4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBZ1t3QHAIDqqaCgYEt3AQA2u5TSlu5CjeLzBEDNUpXzpBHO8A1PPvlkFBQUxJNPPrlZjjdq1KisH8DefvvtKCgoiIkTJ35rx9gQ9913X7Rs2TI+++yzLXL8nPr16xd9+/bd0t0AAAAAqNYEzlRLr776apxwwglRXFwcDRo0iNatW0dJSUncdNNNm70vd911V4wbN67C+vfeey9GjRoVr7zyymbpx/Tp06N79+7RqlWraNSoUey2227Rt2/fePTRRzfL8TfUqlWrYuTIkTF48OBo0qRJRETMmzcvCgsLY+DAgRXaL1myJHbaaac44IADorS0NGtfSkpKoqCgIP793/+9wraCgoJKl6uvvrpcu6FDh8YDDzwQc+fOzdo3AAAAgJpE4Ey188wzz8R+++0Xc+fOjTPPPDN+/vOfxxlnnBF16tSJ8ePHf6vHPvjgg2P58uVx8MEHl61bV+A8evTozRI4X3fddXHMMcdEQUFBDBs2LG688cb413/915g/f37cc88967ztpZdeGsuXL//W+7im6dOnx+uvvx6DBg0qW9exY8e46KKLYuLEifFf//Vf5dpfcskl8be//S1++ctfRp06+f41TZ06NZ599tl1tikpKYnJkyeXW3r37l2uzd577x377bdfXH/99dn6BgAAAFDTqOFMtXPllVdGs2bN4oUXXojmzZuX2/bXv/71Wz12nTp1okGDBt/qMTbUV199FZdffnmUlJTEY489VmH7+h6TunXrRt26m/+tPmHChOjatWu0bt263PrLLrss7r333jjrrLPiT3/6UxQWFsazzz4bt912W5x//vnRqVOnbH1YsWJFXHDBBTF06NAYMWLEWtt997vfjR/+8Ifr3V/fvn1j5MiRcfPNN5eN2gYAAADgH4xwptpZsGBB7LnnnhXC5oiIVq1alft7woQJceihh0arVq2ifv360bFjx7jlllsq3K60tDRGjRoVO++8czRq1CgOOeSQmDdvXrRt2zYGDBhQ1m7NGs4/+MEP4pFHHolFixaVlVpo27ZtPPnkk9G5c+eIiBg4cGDZttW1k5966qk48cQTY9ddd4369etHmzZt4vzzz9+okcYffvhhfPrpp9G1a9dKt6/5mKxpbTWcf/Ob38T+++8fjRo1ihYtWsTBBx9cIdCeMWNGdOvWLRo3bhxNmzaNXr16xZ///Of19nnFihXx6KOPxuGHH15hW4MGDeKWW26J119/PcaOHRsrV66MQYMGRZs2bWLMmDHr3feGuPbaa6O0tDQuvPDC9bZdvnx5rFixYp1tSkpK4vPPP49Zs2bl6iIAAABAjWKEM9VOcXFxPPvss/Haa6/FXnvttc62t9xyS+y5555xzDHHRN26dWP69OlxzjnnRGlpafz4xz8uazds2LC49tpro3fv3tGzZ8+YO3du9OzZc70B4/Dhw+OTTz6Jd955J2688caIiGjSpEnsscceMWbMmBgxYkQMGjQounXrFhERXbp0iYiI+++/P5YtWxZnn312bLvttvH888/HTTfdFO+8807cf//9G/R4tGrVKho2bBjTp0+PwYMHR8uWLTfo9pUZPXp0jBo1Krp06RJjxoyJwsLCeO6552L27NnRo0ePiIiYPHlynHbaadGzZ8+45pprYtmyZXHLLbfEQQcdFC+//HK0bdt2rft/6aWX4ssvv4x99tmn0u0lJSXRv3//GDt2bLz33nvx2muvxcMPPxyNGzcu1+6LL76IpUuXVuk+bbfdduX+Xrx4cVx99dVx5513RsOGDdd524kTJ8bNN98cKaXYY4894tJLL42TTz65QruOHTtGw4YN4+mnn47jjjuuSv0CAAAAqFUSVDOPPfZY2mabbdI222yTvv/976eLL744zZw5M3355ZcV2i5btqzCup49e6bddtut7O/3338/1a1bN/Xp06dcu1GjRqWISKeddlrZujlz5qSISHPmzClb16tXr1RcXFzhOC+88EKKiDRhwoQq9Wvs2LGpoKAgLVq0qGzdyJEjU1XehiNGjEgRkRo3bpyOPPLIdOWVV6aXXnqpQruFCxdW6NOax5g/f36qU6dOOu6449KqVavK3b60tDSllNLSpUtT8+bN05lnnllu+/vvv5+aNWtWYf2a7rjjjhQR6dVXX11rm/fffz+1aNEiRUSF52a1CRMmpIio0rKmE044IXXp0qXs74hIP/7xjyu069KlSxo3blx6+OGH0y233JL22muvFBHp5ptvrrRP3/3ud9ORRx65zvsPNUVV338Wi8VisdSkhby29PNpsVgslrxLVRjhTLVTUlISzz77bIwdOzZmzpwZzz77bFx77bWx/fbbxx133BHHHHNMWdtvjlz95JNPYuXKldG9e/eYOXNmfPLJJ9GsWbN44okn4quvvopzzjmn3HEGDx4co0aN+lbuwzf79fnnn8fy5cujS5cukVKKl19+OXbdddcN2t/o0aOjQ4cOcfPNN8fMmTNjxowZMXz48Nh7771jypQpsccee1R5Xw899FCUlpbGiBEjKkzOt7r0xqxZs2LJkiXRv3//+PDDD8u2b7PNNnHAAQfEnDlz1nmMv//97xER0aJFi7W2adSoUTRq1Cg+/vjjslHVa+rZs+dGla+YM2dOPPDAA/Hcc8+tt+3TTz9d7u/TTz899t133/jpT38aAwYMqDA6ukWLFuUeEwAAAAD+QeBMtdS5c+eYOnVqfPnllzF37tx48MEH48Ybb4wTTjghXnnllejYsWNEfB0Wjhw5Mp599tlYtmxZuX2sDpwXLVoUERG77757ue0tW7ZcZyC6KRYvXhwjRoyIadOmxccff1yhXxujf//+0b9///j000/jueeei4kTJ8Zdd90VvXv3jtdee63Kkx0uWLAg6tSpU/YYVmb+/PkREXHooYdWur2oqKhKx/p6QEPlhg8fHu+//37sscceMXLkyOjXr1+F52OnnXaKnXbaqUrHWu2rr76Kc889N0455ZSyOtsborCwMP793/89fvSjH8VLL70UBx10ULntKaVKa2IDAAAAIHCmmissLIzOnTtH586d47vf/W4MHDgw7r///hg5cmQsWLAgDjvssOjQoUPccMMN0aZNmygsLIzf/e53ceONN0ZpaekW6fOqVauipKQkPvrooxg6dGh06NAhGjduHO+++24MGDBgk/tVVFQUJSUlUVJSEvXq1YtJkybFc889F927d890D6Ksj5MnT44dd9yxwva6ddf9r2PbbbeNiIiPP/44dtlllwrbX3zxxfjFL34R5557bgwcODD23XffGDp0aNx2223l2i1fvrzKAf3qfv7617+O119/PX75y1/G22+/Xa7N0qVL4+23345WrVpFo0aN1rqvNm3aRETERx99VGHbxx9/HN/5zneq1CcAAACA2kbgzFZjv/32i4iIv/zlLxERMX369Pjiiy9i2rRp5UpUrFnuobi4OCIi3nzzzWjXrl3Z+r///e8VRh9XZm2jWde2/tVXX4033ngjJk2aFKeeemrZ+o0pDbE+++23X0yaNKnsMamK9u3bR2lpacybNy86deq01jYRX09YePjhh29wvzp06BAREQsXLox//ud/Lrdt1apVMWjQoNh5551jzJgx0bRp0xgyZEjccMMNMXDgwPj+979f1vbee++NgQMHVumYq0dTL168OFauXBldu3at0ObXv/51/PrXv44HH3ww+vTps9Z9vfXWWxERsf3225db/9VXX8X//d//lSvrAgAAAMA/CJypdubMmRM/+MEPKgS6v/vd7yIi4p/+6Z8i4ut6whHlyzZ88sknMWHChHK3O+yww6Ju3bpxyy23RElJSdn6n//851XqT+PGjSsdZdu4ceOIiFiyZEm59ZX1K6UU48ePr9Lx1rRs2bKYO3duuSB2tRkzZkTEPx6TqujTp08MHTo0xowZE7/97W/L1XFeXS6iZ8+eUVRUFFdddVUccsghUa9evXL7+Nvf/lYhjP2mfffdNwoLC+PFF1+sEM7+x3/8R7z88ssxderUaNq0aUR8XaP6vvvuKytjsXoE9cbUcO7Xr1+lQfpxxx0XRx11VJx55plxwAEHrPV+LF26NMaNGxfbbbdd7LvvvuW2zZs3L1asWBFdunTZoD4BAAAA1BYCZ6qdwYMHx7Jly+K4446LDh06xJdffhnPPPNM3HvvvdG2bduyEa89evSIwsLC6N27d5x11lnx2Wefxe233x6tWrUqN+J3hx12iCFDhsT1118fxxxzTBxxxBExd+7cmDFjRmy33Xbrrce77777xr333hs/+clPonPnztGkSZPo3bt3tG/fPpo3bx633nprNG3aNBo3bhwHHHBAdOjQIdq3bx8XXnhhvPvuu1FUVBQPPPBAlUZTV2bZsmXRpUuXOPDAA+OII46INm3axJIlS+Khhx6Kp556Kvr06RN77713lfe3++67x/Dhw+Pyyy+Pbt26xfHHHx/169ePF154IXbeeecYO3ZsFBUVxS233BKnnHJK7LPPPtGvX7/YfvvtY/HixfHII49E165d1xnYN2jQIHr06BGPP/54jBkzpmz9//3f/8WIESOid+/ecdxxx5Wtb9y4cYwfPz6OP/74GD9+fFxwwQURsXE1nDt06FA2wnpN7dq1Kzey+Re/+EU89NBD0bt379h1113jL3/5S9x5552xePHimDx5chQWFpa7/axZs6JRo0blfrgAAAAA4BsSVDMzZsxIp59+eurQoUNq0qRJKiwsTLvvvnsaPHhw+uCDD8q1nTZtWvre976XGjRokNq2bZuuueaadOedd6aISAsXLixr99VXX6XLLrss7bjjjqlhw4bp0EMPTf/zP/+Ttt122/SjH/2orN2cOXNSRKQ5c+aUrfvss8/SySefnJo3b54iIhUXF5dte/jhh1PHjh1T3bp1U0SkCRMmpJRSmjdvXjr88MNTkyZN0nbbbZfOPPPMNHfu3HJtUkpp5MiRaX1vw5UrV6bbb7899enTJxUXF6f69eunRo0apb333jv97Gc/S1988UVZ24ULF1b5GHfeeWfae++9U/369VOLFi1S9+7d06xZs8q1mTNnTurZs2dq1qxZatCgQWrfvn0aMGBAevHFF9fZ55RSmjp1aiooKEiLFy8uW3fsscemxo0bp0WLFlV6m6OPPjo1adKk3G1yiYj04x//uNy6xx57LJWUlKQdd9wx1atXLzVv3jz16NEjPfHEE5Xu44ADDkg//OEPs/cNqquIsFgsFoul1i3ktaWfT4vFYrHkXaqi4P8/AUCts2TJkmjRokVcccUVMXz48C3dnRpn1apV0bFjx+jbt29cfvnlW7o7m+yVV16JffbZJ/74xz+utfY11DTruwIEAGoiX5Hz8nkCoGapynmyznpbQA2wfPnyCuvGjRsXERE/+MEPNm9naoltttkmxowZE7/4xS/is88+29Ld2WRXX311nHDCCcJmAAAAgHUwwplaYeLEiTFx4sQ46qijokmTJvGHP/wh7r777ujRo0fMnDlzS3cPoFoyIgmA2shX5Lx8ngCoWapynjRpILXC9773vahbt25ce+218emnn5ZNJHjFFVds6a4BAAAAQI1hhDMAUCkjkgCojXxFzsvnCYCaRQ1nAAAAAAA2G4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBZ1q9pQoX8A2PxMXAQAAMDWxAhnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBYCZwAAAAAAshA4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBYCZwAAAAAAshA4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBYCZwAAAAAAshA4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZ1N3SHaB2Sylt8j4KCgoy9AQAAAAA2FRGOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALkway1avKxIMmFgQAAACAb58RzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZqOHMFrVmbeWq1GPeGJXtV11nAAAAAMjLCGcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFnW3dAeo3VJKm7yPgoKCDD0BAAAAADaVEc4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALOpu6Q6wdUgpbekurNXG9q2goCBzTwAAAACgdjPCGQAAAACALATOAAAAAABkIXAGAAAAACALNZyptb6tutRqQwMAAABQWxnhDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCxMGkiVbMmJ8L6tyf0AAAAAgLyMcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQg3nGk7947XbknWpAQAAAKAmMsIZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkEXdLd0B2BwKCgq2dBcAAAAAoMYzwhkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgCzWct2IppS3dBQAAAACAMkY4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJBF3S3dATZeQUFBhXUppS3Qk+qvKo9LZY8nAAAAAFB1RjgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZKGGcw2zZh1iNZ0BAAAAgM3FCGcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkYdLAGs4kgl9b83EAAAAAAPIzwhkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgCzWca5mq1DKuCXWe1WwGAAAAgM3PCGcAAAAAALIQOAMAAAAAkIXAGQAAAACALNRwpkaqznWo1ZcGAAAAoKYywhkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZmDSQCiqb1K46T8IHAAAAAFQPRjgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZKGGM3zLKquJDQAAAAA1kRHOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyMKkgVTJmhPfpZS2UE+qFxMCAgAAAMA/GOEMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIUazmyUymoX14a6zmo2AwAAAMDaGeEMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIUazrAW6jUDAAAAwIYxwhkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZmDSQrY7J/AAAAACgejLCGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJBF3S3dAWqOgoKCcn+nlLZQTwAAAACALcEIZwAAAAAAshA4AwAAAACQhcAZAAAAAIAs1HDmW7NmTefKqPMMAADVy6Z8Rq/KdwAAoGYzwhkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZmDSQLcqkIgBUV9V5YlvnTyCX3P/rKtuf/1kAULsY4QwAAAAAQBZGOAMAAAAAbGZrXhlUU64KMsIZAAAAAIAsBM4AAAAAAGShpAYAUOtV5wkCAb7J/ysAoLoTOAMAAABUUXX84aem1H2F6qw6vverKyU1AAAAAADIQuAMAAAAAEAWAmcAAAAAALJQwxkAoBpTkxFqL7UiAYCtkcAZAAAAYCtW2Q9UfrSmtvPD7ZajpAYAAAAAAFkInAEAAAAAyELgDAAAAABAFmo4AwC1ytZWy62q/VWnEbYuW9v/IqjNttb365r99lmBmm5rfa/WREY4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGShhjMAAABADaemM9WZ+ss1i8AZAKAGMLkgVF++RAMAtYmSGgAAAAAAZCFwBgAAAAAgCyU1AAAAAGoZNZ2pTtZ8/SlHtXUzwhkAAAAAgCyMcAYAqEUqGy1iRBOsn5FWAABVY4QzAAAAAABZGOEMAAAAUMup6Ux1sr7XnyuPqjcjnAEAAAAAyELgDAAAAABAFkpqAAA1lkvtqsZEggAAQC4CZwAAAIBazo/NX9vQAQtrPm7fxoAHz01FVXlMDD7ZcpTUAAAAAAAgC4EzAAAAAABZCJwBAAAAAMhCDWcAAACAWkZd4K9tDXV+1+yj567mqKnPpRHOAAAAAABkIXAGAAAAACALgTMAAAAAAFmo4QwAAABQw9XUWrEbYmuo11wVtbGmc0157moLgTMAQC1XG76kwKaq7H3iy29F/p8AAEpqAAAAAACQhcAZAAAAAIAslNQAAAAAqGGUuNky1nzclV+iNjLCGQAAAACALIxwBgAA2AhVHT1Ym0a3VXZfjbIEgNrFCGcAAAAAALIQOAMAAAAAkIWSGgAAAACwlVqznJFSRtVXbXlujHAGAAAAACALI5wBAGqo2jKCAiqzpSavq00TBAIAVMYIZwAAAAAAsjDCGQAAAGAt1rw6Ymu5kkFd363nucptffe7Or4WautzVVMZ4QwAAAAAQBZGOAMANVZlozdq6uiJ6jhSBQAAqH2McAYAAAAAIAsjnAEAAABquA29ymtrvHqqpl7Jlpv63nzbjHAGAAAAACALgTMAAAAAAFkoqQEAQJVUl8tUXfZZM2yJ11NVj7kpr7Gq3ra6vJ8AAHITOAMAAABUUWU/LNWEH5G2xh90q8PjvjXUQ66OfaJmU1IDAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAs1nAEAaoDKahjW1Hp9m2PiN/KqDjU2N4TXGLCh1vx/sLX936NmcX6qPmrrc2GEMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWajgDAAAAsNVZX33cLVFL+9s4Zm2tA8zWS+AMAACwmVUWHphkDACoCZTUAAAAAAAgC4EzAAAAAABZKKkBAAAAQI2zZvmizVG6SL3l2stz/w9GOAMAAAAAkIURzgAANYARFRVVNorJ4/TtM/EdAEDtZoQzAAAAAABZGOEMAAAAkNGWqB3M+lXlSifPFWw6I5wBAAAAAMhC4AwAAAAAQBZKagAAtUpll1LW1EsnN+V+1dTJ9UwkWL1Vl0udq/qaqGpfvMYAgNpE4AwAAAAAseE/EtbUgQusnx+U105JDQAAAAAAshA4AwAAAACQhcAZAAAAAIAs1HAGAKgBtsaJ1DblGDmPycbL/bivub/KXg+b43WYW3Xqy7dNPUsAQOAMAAAA8C1a88eY2vRDVE3nhzaoSEkNAAAAAACyEDgDAAAAAJCFwBkAAAAAgCzUcAYAar3Kau+prbjxNuXx9LjzTdVpMkyAnNR0hvK8J2oWI5wBAAAAAMhC4AwAAAAAQBYCZwAAAAAAslDDGQAAAABgHcz7UHUCZwAAag1fFPKq6uNp4p+ayfsJAKiMkhoAAAAAAGQhcAYAAAAAIAslNQAAAAC2oDVL1GyOUkTK4lCdbYn3xPr6QNUZ4QwAAAAAQBZGOAMAUEFlo0g2ZZRHZbc1kRxbO69rAICKjHAGAAAAACALI5wBAAAAqpFvo36terRszary+t3U94n3SD5GOAMAAAAAkIXAGQAAAACALJTUAACoxKZcUmfSsKrJeWmkSyCrt+oyuZ7XCQDAt0/gDAAAAFCNbUxNZz+yUdt4zVcfSmoAAAAAAJCFwBkAAAAAgCwEzgAAAAAAZKGGMwBAZtVlgrSaSn2+msHzCLDx/A8FqjMjnAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFSQMBAKigukxGVF36ATWB9xMAsDkY4QwAAAAAQBYCZwAAAAAAshA4AwAAAACQhRrOAACbgdqpwPr4PwEA1ARGOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALkwYCAABkYuI/AKC2M8IZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBYCZwAAAAAAshA4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgC4EzAAAAAABZCJwBAAAAAMhC4AwAAAAAQBYCZwAAAAAAshA4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCzqVrVhSunb7AcAAAAAAFs5I5wBAAAAAMhC4AwAAAAAQBYCZwAAAAAAshA4AwAAAACQhcAZAAAAAIAsBM4AAAAAAGQhcAYAAAAAIAuBMwAAAAAAWQicAQAAAADIQuAMAAAAAEAWAmcAAAAAALIQOAMAAAAAkIXAGQAAAACALATOAAAAAABkIXAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZFGQUkpbuhMAAAAAAGz9jHAGAAAAACALgTMAAAAAAFkInAEAAAAAyELgDAAAAABAFgJnAAAAAACyEDgDAAAAAJCFwBkAAAAAgCwEzgAAAAAAZCFwBgAAAAAgi/8PiKqh8dZAtnUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up ROI mask to be used in the guidance loss function\n",
    "mask_path = Path(\"/home/diogommiranda/tese/masks/ROI_MASK.nii\")\n",
    "mask_img = nib.load(mask_path)\n",
    "mask_volume = mask_img.get_fdata(dtype=np.float32)\n",
    "assert mask_volume.shape == (91, 109, 91), \"Mask volume shape must be (91, 109, 91)\"\n",
    "if apply_padding:\n",
    "    padding = (\n",
    "        (2,3), # Pad width from 91 to 96\n",
    "        (1,2), # Pad height from 109 to 112\n",
    "        (2,3)  # Pad depth from 91 to 96\n",
    "    )\n",
    "    mask_volume = np.pad(mask_volume, padding, mode='edge')\n",
    "    \n",
    "mask_volume = np.expand_dims(mask_volume, axis=0)\n",
    "mask_volume = np.transpose(mask_volume, (0, 3, 2, 1)) # Transpose to (depth, height, width)\n",
    "print(f\"Mask volume shape: {mask_volume.shape}\")\n",
    "mask_volume = torch.tensor(mask_volume, dtype=torch.float32)\n",
    "view_image_data(mask_volume)\n",
    "\n",
    "# Get number of ROI and non-ROI voxels in the mask - needed for the guidance loss function\n",
    "roi_voxels = mask_volume[mask_volume == 1].numel()\n",
    "print(f\"Number of ROI voxels: {roi_voxels}\")\n",
    "non_roi_voxels = mask_volume[mask_volume == 0].numel()\n",
    "print(f\"Number of non-ROI voxels: {non_roi_voxels}\")\n",
    "\n",
    "mask_volume = mask_volume.to(device)\n",
    "print(f\"Mask volume shape after adding batch dimension: {mask_volume.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec7cd6",
   "metadata": {},
   "source": [
    "# Select guidance loss config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938d08d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw logits attention maps in the guidance loss.\n",
      "Using image-level guidance loss.\n"
     ]
    }
   ],
   "source": [
    "# ----- Hyperparameter Tuning with K-Fold Cross-Validation -----\n",
    "RETURN_ATTENTION = 'raw' # Choose attention type: 'softmaxed', 'raw', 'none'\n",
    "LEVEL = 'image' # Choose level-space to apply guidance loss: 'image', 'token'\n",
    "#LOSS_FUNCTION = 'kvl' # choose loss function: 'kvl', 'weighted_sum'\n",
    "\n",
    "if RETURN_ATTENTION == 'softmaxed':\n",
    "    print(\"Using softmaxed attention maps in the guidance loss.\")\n",
    "elif RETURN_ATTENTION == 'raw':\n",
    "    print(\"Using raw logits attention maps in the guidance loss.\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid attention type. Use 'softmaxed' or 'raw' to retrieve the attention maps.\")\n",
    "\n",
    "if LEVEL == 'image':\n",
    "    print(\"Using image-level guidance loss.\")\n",
    "    mask_volume.to(device) # Move mask volume to device\n",
    "if LEVEL == 'token':\n",
    "    # Interpolate the mask volume to match the \n",
    "    # add batch dimension\n",
    "    mask_volume = mask_volume.unsqueeze(0) # Add batch dimension\n",
    "    mask_volume = F.interpolate(\n",
    "    input = mask_volume,\n",
    "    mode = 'area',\n",
    "    size = (6, 7, 6)\n",
    "    )\n",
    "    token_roi_scores = mask_volume.flatten(start_dim=1)\n",
    "    token_roi_scores = token_roi_scores.to(device) # Move token ROI scores to device\n",
    "elif LEVEL != 'image':\n",
    "    raise ValueError(\"Invalid level-space. Use 'image' or 'token'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055270c",
   "metadata": {},
   "source": [
    "# Nested Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88ee30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Combination: lambda=0.0001 for CLS_LAYER=6 and hybrid model\n",
      "    Saving results for this combo to: /home/diogommiranda/tese/outputs/roi_guidance/resnetvit/smci_pmci_balanced/Layer_6/T=0.1/lambda_0.0001\n",
      "============================================================\n",
      "------------------------------------------------------------\n",
      "Training Fold 1/5...\n",
      "  Train samples: 513, Validation samples: 150\n",
      "Calculated Min: -0.2443523108959198, Max: 2.7912356853485107\n",
      "  Fold train label counts: {0: 272, 1: 241}\n",
      "  Calculated pos_weight for BCEWithLogitsLoss: 1.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:06<10:48,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1/100 [6.54s] - Train Loss: 51.84522, Train Acc: 0.49708Train classification Loss: 0.74998, Train Guidance Loss: 51.09524, Train Penalization Term Loss: -0.95183, Train Reward Term Loss: -510951.49744 | Val Loss: 51.83884, Val Acc: 0.47333Val classification Loss: 0.73710, Val Guidance Loss: 51.10174, Val Penalization Term Loss: -0.95188, Val Reward Term Loss: -511016.44458 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 2/100 [00:16<14:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2/100 [9.99s] - Train Loss: 51.81134, Train Acc: 0.51267Train classification Loss: 0.73734, Train Guidance Loss: 51.07401, Train Penalization Term Loss: -0.95161, Train Reward Term Loss: -510739.14315 | Val Loss: 51.81730, Val Acc: 0.48667Val classification Loss: 0.73452, Val Guidance Loss: 51.08278, Val Penalization Term Loss: -0.95171, Val Reward Term Loss: -510826.84292 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 3/100 [00:23<12:35,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3/100 [6.86s] - Train Loss: 51.79749, Train Acc: 0.53021Train classification Loss: 0.73386, Train Guidance Loss: 51.06363, Train Penalization Term Loss: -0.95154, Train Reward Term Loss: -510635.40765 | Val Loss: 51.82570, Val Acc: 0.58667Val classification Loss: 0.73358, Val Guidance Loss: 51.09212, Val Penalization Term Loss: -0.95191, Val Reward Term Loss: -510920.25583 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 4/100 [00:30<11:59,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4/100 [7.03s] - Train Loss: 51.78062, Train Acc: 0.53606Train classification Loss: 0.73268, Train Guidance Loss: 51.04794, Train Penalization Term Loss: -0.95143, Train Reward Term Loss: -510478.51693 | Val Loss: 51.80831, Val Acc: 0.48667Val classification Loss: 0.73215, Val Guidance Loss: 51.07616, Val Penalization Term Loss: -0.95174, Val Reward Term Loss: -510760.62667 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 5/100 [00:37<11:36,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5/100 [7.06s] - Train Loss: 51.78110, Train Acc: 0.50292Train classification Loss: 0.73038, Train Guidance Loss: 51.05072, Train Penalization Term Loss: -0.95145, Train Reward Term Loss: -510506.27705 | Val Loss: 51.81356, Val Acc: 0.52000Val classification Loss: 0.73294, Val Guidance Loss: 51.08063, Val Penalization Term Loss: -0.95181, Val Reward Term Loss: -510805.36833 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 6/100 [00:44<11:24,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 6/100 [7.18s] - Train Loss: 51.77173, Train Acc: 0.51267Train classification Loss: 0.73034, Train Guidance Loss: 51.04139, Train Penalization Term Loss: -0.95139, Train Reward Term Loss: -510412.98173 | Val Loss: 51.80522, Val Acc: 0.51333Val classification Loss: 0.73425, Val Guidance Loss: 51.07097, Val Penalization Term Loss: -0.95181, Val Reward Term Loss: -510708.80500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 7/100 [00:56<13:31,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7/100 [11.70s] - Train Loss: 51.75887, Train Acc: 0.52632Train classification Loss: 0.72749, Train Guidance Loss: 51.03138, Train Penalization Term Loss: -0.95119, Train Reward Term Loss: -510312.91545 | Val Loss: 51.79594, Val Acc: 0.51333Val classification Loss: 0.73150, Val Guidance Loss: 51.06443, Val Penalization Term Loss: -0.95165, Val Reward Term Loss: -510643.39750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 8/100 [01:03<12:41,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 8/100 [7.32s] - Train Loss: 51.75891, Train Acc: 0.59649Train classification Loss: 0.72971, Train Guidance Loss: 51.02919, Train Penalization Term Loss: -0.95119, Train Reward Term Loss: -510291.02132 | Val Loss: 51.76796, Val Acc: 0.57333Val classification Loss: 0.72556, Val Guidance Loss: 51.04240, Val Penalization Term Loss: -0.95151, Val Reward Term Loss: -510423.06750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 9/100 [01:10<11:51,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 9/100 [6.82s] - Train Loss: 51.74464, Train Acc: 0.52632Train classification Loss: 0.72537, Train Guidance Loss: 51.01928, Train Penalization Term Loss: -0.95116, Train Reward Term Loss: -510191.84917 | Val Loss: 51.77884, Val Acc: 0.51333Val classification Loss: 0.72840, Val Guidance Loss: 51.05044, Val Penalization Term Loss: -0.95154, Val Reward Term Loss: -510503.48500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 10/100 [01:17<11:17,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10/100 [6.88s] - Train Loss: 51.73900, Train Acc: 0.61209Train classification Loss: 0.72356, Train Guidance Loss: 51.01544, Train Penalization Term Loss: -0.95107, Train Reward Term Loss: -510153.46193 | Val Loss: 51.74733, Val Acc: 0.74667Val classification Loss: 0.72301, Val Guidance Loss: 51.02432, Val Penalization Term Loss: -0.95127, Val Reward Term Loss: -510242.28333 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 11/100 [01:27<12:25,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 11/100 [10.27s] - Train Loss: 51.72503, Train Acc: 0.61793Train classification Loss: 0.72164, Train Guidance Loss: 51.00340, Train Penalization Term Loss: -0.95106, Train Reward Term Loss: -510033.05775 | Val Loss: 51.73323, Val Acc: 0.48667Val classification Loss: 0.72091, Val Guidance Loss: 51.01232, Val Penalization Term Loss: -0.95098, Val Reward Term Loss: -510122.31208 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 12/100 [01:34<11:30,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 12/100 [6.66s] - Train Loss: 51.70843, Train Acc: 0.58090Train classification Loss: 0.72167, Train Guidance Loss: 50.98677, Train Penalization Term Loss: -0.95079, Train Reward Term Loss: -509866.73307 | Val Loss: 51.73216, Val Acc: 0.73333Val classification Loss: 0.71965, Val Guidance Loss: 51.01252, Val Penalization Term Loss: -0.95106, Val Reward Term Loss: -510124.23917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 13/100 [01:40<10:51,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 13/100 [6.65s] - Train Loss: 51.69371, Train Acc: 0.61793Train classification Loss: 0.71616, Train Guidance Loss: 50.97755, Train Penalization Term Loss: -0.95084, Train Reward Term Loss: -509774.59887 | Val Loss: 51.70349, Val Acc: 0.60667Val classification Loss: 0.71571, Val Guidance Loss: 50.98778, Val Penalization Term Loss: -0.95090, Val Reward Term Loss: -509876.82583 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 14/100 [01:47<10:23,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 14/100 [6.69s] - Train Loss: 51.67890, Train Acc: 0.61793Train classification Loss: 0.71572, Train Guidance Loss: 50.96319, Train Penalization Term Loss: -0.95053, Train Reward Term Loss: -509630.93385 | Val Loss: 51.69913, Val Acc: 0.52000Val classification Loss: 0.71412, Val Guidance Loss: 50.98501, Val Penalization Term Loss: -0.95097, Val Reward Term Loss: -509849.16958 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 15/100 [01:54<10:01,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 15/100 [6.68s] - Train Loss: 51.66016, Train Acc: 0.61014Train classification Loss: 0.71102, Train Guidance Loss: 50.94914, Train Penalization Term Loss: -0.95049, Train Reward Term Loss: -509490.43573 | Val Loss: 51.69331, Val Acc: 0.63333Val classification Loss: 0.71401, Val Guidance Loss: 50.97930, Val Penalization Term Loss: -0.95090, Val Reward Term Loss: -509792.08750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 16/100 [02:04<11:14,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 16/100 [10.25s] - Train Loss: 51.65912, Train Acc: 0.74464Train classification Loss: 0.70703, Train Guidance Loss: 50.95209, Train Penalization Term Loss: -0.95052, Train Reward Term Loss: -509519.98666 | Val Loss: 51.65535, Val Acc: 0.66667Val classification Loss: 0.70725, Val Guidance Loss: 50.94811, Val Penalization Term Loss: -0.95035, Val Reward Term Loss: -509480.12750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 17/100 [02:11<10:35,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 17/100 [6.77s] - Train Loss: 51.62368, Train Acc: 0.64717Train classification Loss: 0.70309, Train Guidance Loss: 50.92059, Train Penalization Term Loss: -0.95015, Train Reward Term Loss: -509204.93707 | Val Loss: 51.65051, Val Acc: 0.73333Val classification Loss: 0.70600, Val Guidance Loss: 50.94452, Val Penalization Term Loss: -0.95027, Val Reward Term Loss: -509444.26917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 18/100 [02:18<10:03,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 18/100 [6.67s] - Train Loss: 51.61594, Train Acc: 0.74659Train classification Loss: 0.70123, Train Guidance Loss: 50.91471, Train Penalization Term Loss: -0.95001, Train Reward Term Loss: -509146.19981 | Val Loss: 51.62045, Val Acc: 0.80667Val classification Loss: 0.70014, Val Guidance Loss: 50.92031, Val Penalization Term Loss: -0.95004, Val Reward Term Loss: -509202.18708 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 19/100 [02:24<09:40,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 19/100 [6.70s] - Train Loss: 51.58932, Train Acc: 0.74269Train classification Loss: 0.69771, Train Guidance Loss: 50.89161, Train Penalization Term Loss: -0.94972, Train Reward Term Loss: -508915.15406 | Val Loss: 51.61268, Val Acc: 0.78000Val classification Loss: 0.69585, Val Guidance Loss: 50.91683, Val Penalization Term Loss: -0.95020, Val Reward Term Loss: -509167.42000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 20/100 [02:31<09:21,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 20/100 [6.67s] - Train Loss: 51.56730, Train Acc: 0.68811Train classification Loss: 0.69378, Train Guidance Loss: 50.87352, Train Penalization Term Loss: -0.94951, Train Reward Term Loss: -508734.29758 | Val Loss: 51.59970, Val Acc: 0.69333Val classification Loss: 0.69499, Val Guidance Loss: 50.90471, Val Penalization Term Loss: -0.94989, Val Reward Term Loss: -509046.21667 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 21/100 [02:41<10:26,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 21/100 [10.07s] - Train Loss: 51.54671, Train Acc: 0.75634Train classification Loss: 0.68708, Train Guidance Loss: 50.85963, Train Penalization Term Loss: -0.94940, Train Reward Term Loss: -508595.39711 | Val Loss: 51.56502, Val Acc: 0.80667Val classification Loss: 0.68537, Val Guidance Loss: 50.87964, Val Penalization Term Loss: -0.94978, Val Reward Term Loss: -508795.50458 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 22/100 [02:47<09:43,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 22/100 [6.42s] - Train Loss: 51.50541, Train Acc: 0.80507Train classification Loss: 0.67585, Train Guidance Loss: 50.82956, Train Penalization Term Loss: -0.94905, Train Reward Term Loss: -508294.70608 | Val Loss: 51.52332, Val Acc: 0.76000Val classification Loss: 0.67498, Val Guidance Loss: 50.84835, Val Penalization Term Loss: -0.94927, Val Reward Term Loss: -508482.54833 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 23/100 [02:54<09:13,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 23/100 [6.49s] - Train Loss: 51.47587, Train Acc: 0.72125Train classification Loss: 0.66904, Train Guidance Loss: 50.80683, Train Penalization Term Loss: -0.94871, Train Reward Term Loss: -508067.40229 | Val Loss: 51.47238, Val Acc: 0.60000Val classification Loss: 0.67387, Val Guidance Loss: 50.79851, Val Penalization Term Loss: -0.94853, Val Reward Term Loss: -507984.13958 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 24/100 [03:01<08:52,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 24/100 [6.57s] - Train Loss: 51.45158, Train Acc: 0.77973Train classification Loss: 0.66573, Train Guidance Loss: 50.78585, Train Penalization Term Loss: -0.94832, Train Reward Term Loss: -507857.54557 | Val Loss: 51.45763, Val Acc: 0.77333Val classification Loss: 0.65623, Val Guidance Loss: 50.80139, Val Penalization Term Loss: -0.94852, Val Reward Term Loss: -508013.01458 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 25/100 [03:07<08:37,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 25/100 [6.66s] - Train Loss: 51.39499, Train Acc: 0.77388Train classification Loss: 0.64789, Train Guidance Loss: 50.74710, Train Penalization Term Loss: -0.94788, Train Reward Term Loss: -507470.06530 | Val Loss: 51.39280, Val Acc: 0.78000Val classification Loss: 0.64308, Val Guidance Loss: 50.74972, Val Penalization Term Loss: -0.94768, Val Reward Term Loss: -507496.31250 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 26/100 [03:17<09:45,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 26/100 [10.29s] - Train Loss: 51.35713, Train Acc: 0.78558Train classification Loss: 0.63043, Train Guidance Loss: 50.72669, Train Penalization Term Loss: -0.94746, Train Reward Term Loss: -507266.01212 | Val Loss: 51.33530, Val Acc: 0.73333Val classification Loss: 0.62779, Val Guidance Loss: 50.70751, Val Penalization Term Loss: -0.94698, Val Reward Term Loss: -507074.20750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 27/100 [03:24<09:12,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 27/100 [6.76s] - Train Loss: 51.29745, Train Acc: 0.80507Train classification Loss: 0.61634, Train Guidance Loss: 50.68111, Train Penalization Term Loss: -0.94681, Train Reward Term Loss: -506810.17641 | Val Loss: 51.28834, Val Acc: 0.80000Val classification Loss: 0.60749, Val Guidance Loss: 50.68085, Val Penalization Term Loss: -0.94653, Val Reward Term Loss: -506807.53333 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 28/100 [03:31<08:49,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 28/100 [6.85s] - Train Loss: 51.21754, Train Acc: 0.84990Train classification Loss: 0.58267, Train Guidance Loss: 50.63488, Train Penalization Term Loss: -0.94604, Train Reward Term Loss: -506347.86245 | Val Loss: 51.25668, Val Acc: 0.82667Val classification Loss: 0.58511, Val Guidance Loss: 50.67157, Val Penalization Term Loss: -0.94637, Val Reward Term Loss: -506714.79417 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 29/100 [03:38<08:34,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 29/100 [6.99s] - Train Loss: 51.16393, Train Acc: 0.83821Train classification Loss: 0.55919, Train Guidance Loss: 50.60474, Train Penalization Term Loss: -0.94561, Train Reward Term Loss: -506046.48081 | Val Loss: 51.18956, Val Acc: 0.82667Val classification Loss: 0.55930, Val Guidance Loss: 50.63026, Val Penalization Term Loss: -0.94556, Val Reward Term Loss: -506301.63417 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 30/100 [03:49<09:36,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 30/100 [10.54s] - Train Loss: 51.11031, Train Acc: 0.80897Train classification Loss: 0.53716, Train Guidance Loss: 50.57315, Train Penalization Term Loss: -0.94491, Train Reward Term Loss: -505730.56981 | Val Loss: 51.11926, Val Acc: 0.82000Val classification Loss: 0.52842, Val Guidance Loss: 50.59084, Val Penalization Term Loss: -0.94485, Val Reward Term Loss: -505907.51833 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 31/100 [03:55<08:59,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 31/100 [6.86s] - Train Loss: 51.07339, Train Acc: 0.78558Train classification Loss: 0.53292, Train Guidance Loss: 50.54046, Train Penalization Term Loss: -0.94420, Train Reward Term Loss: -505403.69895 | Val Loss: 51.07394, Val Acc: 0.81333Val classification Loss: 0.50614, Val Guidance Loss: 50.56780, Val Penalization Term Loss: -0.94419, Val Reward Term Loss: -505677.10500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 32/100 [04:02<08:33,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 32/100 [6.93s] - Train Loss: 51.00925, Train Acc: 0.81871Train classification Loss: 0.48970, Train Guidance Loss: 50.51955, Train Penalization Term Loss: -0.94372, Train Reward Term Loss: -505194.54228 | Val Loss: 51.00025, Val Acc: 0.81333Val classification Loss: 0.48412, Val Guidance Loss: 50.51613, Val Penalization Term Loss: -0.94304, Val Reward Term Loss: -505160.41500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 33/100 [04:09<08:14,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 33/100 [6.96s] - Train Loss: 50.95015, Train Acc: 0.81676Train classification Loss: 0.46512, Train Guidance Loss: 50.48502, Train Penalization Term Loss: -0.94301, Train Reward Term Loss: -504849.31439 | Val Loss: 50.98036, Val Acc: 0.81333Val classification Loss: 0.46669, Val Guidance Loss: 50.51367, Val Penalization Term Loss: -0.94271, Val Reward Term Loss: -505135.79500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 34/100 [04:16<07:58,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 34/100 [6.94s] - Train Loss: 50.91830, Train Acc: 0.83431Train classification Loss: 0.44276, Train Guidance Loss: 50.47555, Train Penalization Term Loss: -0.94257, Train Reward Term Loss: -504754.56207 | Val Loss: 50.95717, Val Acc: 0.80000Val classification Loss: 0.45133, Val Guidance Loss: 50.50584, Val Penalization Term Loss: -0.94272, Val Reward Term Loss: -505057.45917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 35/100 [04:27<08:50,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 35/100 [10.27s] - Train Loss: 50.86251, Train Acc: 0.83821Train classification Loss: 0.39648, Train Guidance Loss: 50.46603, Train Penalization Term Loss: -0.94240, Train Reward Term Loss: -504659.35459 | Val Loss: 50.93556, Val Acc: 0.82667Val classification Loss: 0.44159, Val Guidance Loss: 50.49397, Val Penalization Term Loss: -0.94234, Val Reward Term Loss: -504938.76833 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 36/100 [04:33<08:15,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 36/100 [6.75s] - Train Loss: 50.82335, Train Acc: 0.84211Train classification Loss: 0.38707, Train Guidance Loss: 50.43628, Train Penalization Term Loss: -0.94186, Train Reward Term Loss: -504361.90802 | Val Loss: 50.88418, Val Acc: 0.82000Val classification Loss: 0.43297, Val Guidance Loss: 50.45121, Val Penalization Term Loss: -0.94147, Val Reward Term Loss: -504511.19292 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 37/100 [04:40<07:47,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 37/100 [6.71s] - Train Loss: 50.79956, Train Acc: 0.85380Train classification Loss: 0.39409, Train Guidance Loss: 50.40547, Train Penalization Term Loss: -0.94121, Train Reward Term Loss: -504053.81518 | Val Loss: 50.88580, Val Acc: 0.83333Val classification Loss: 0.43319, Val Guidance Loss: 50.45261, Val Penalization Term Loss: -0.94139, Val Reward Term Loss: -504525.21667 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 38/100 [04:47<07:27,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 38/100 [6.73s] - Train Loss: 50.81289, Train Acc: 0.83626Train classification Loss: 0.40771, Train Guidance Loss: 50.40517, Train Penalization Term Loss: -0.94121, Train Reward Term Loss: -504050.78710 | Val Loss: 50.85548, Val Acc: 0.82667Val classification Loss: 0.42699, Val Guidance Loss: 50.42849, Val Penalization Term Loss: -0.94097, Val Reward Term Loss: -504283.98375 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 39/100 [04:54<07:11,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 39/100 [6.76s] - Train Loss: 50.74284, Train Acc: 0.84990Train classification Loss: 0.37274, Train Guidance Loss: 50.37011, Train Penalization Term Loss: -0.94062, Train Reward Term Loss: -503700.14742 | Val Loss: 50.84311, Val Acc: 0.83333Val classification Loss: 0.42584, Val Guidance Loss: 50.41727, Val Penalization Term Loss: -0.94098, Val Reward Term Loss: -504171.82375 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 40/100 [05:04<08:04,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 40/100 [10.42s] - Train Loss: 50.73089, Train Acc: 0.86160Train classification Loss: 0.36101, Train Guidance Loss: 50.36987, Train Penalization Term Loss: -0.94059, Train Reward Term Loss: -503697.81445 | Val Loss: 50.83108, Val Acc: 0.79333Val classification Loss: 0.42639, Val Guidance Loss: 50.40469, Val Penalization Term Loss: -0.94080, Val Reward Term Loss: -504045.99250 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 41/100 [05:11<07:39,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 41/100 [7.08s] - Train Loss: 50.73600, Train Acc: 0.84211Train classification Loss: 0.37904, Train Guidance Loss: 50.35696, Train Penalization Term Loss: -0.94045, Train Reward Term Loss: -503568.72015 | Val Loss: 50.81992, Val Acc: 0.82000Val classification Loss: 0.41860, Val Guidance Loss: 50.40132, Val Penalization Term Loss: -0.94064, Val Reward Term Loss: -504012.24625 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 42/100 [05:18<07:17,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 42/100 [6.98s] - Train Loss: 50.69785, Train Acc: 0.85185Train classification Loss: 0.33493, Train Guidance Loss: 50.36292, Train Penalization Term Loss: -0.94040, Train Reward Term Loss: -503628.25560 | Val Loss: 50.80913, Val Acc: 0.79333Val classification Loss: 0.42696, Val Guidance Loss: 50.38217, Val Penalization Term Loss: -0.94033, Val Reward Term Loss: -503820.73750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 43/100 [05:25<06:57,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 43/100 [6.82s] - Train Loss: 50.69041, Train Acc: 0.85185Train classification Loss: 0.35543, Train Guidance Loss: 50.33498, Train Penalization Term Loss: -0.93987, Train Reward Term Loss: -503348.83656 | Val Loss: 50.81738, Val Acc: 0.81333Val classification Loss: 0.42890, Val Guidance Loss: 50.38848, Val Penalization Term Loss: -0.94044, Val Reward Term Loss: -503883.89833 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 44/100 [05:35<07:44,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 44/100 [10.54s] - Train Loss: 50.67171, Train Acc: 0.87135Train classification Loss: 0.35260, Train Guidance Loss: 50.31910, Train Penalization Term Loss: -0.93968, Train Reward Term Loss: -503190.08175 | Val Loss: 50.78957, Val Acc: 0.82000Val classification Loss: 0.41934, Val Guidance Loss: 50.37022, Val Penalization Term Loss: -0.94002, Val Reward Term Loss: -503701.28167 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 45/100 [05:42<07:14,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 45/100 [6.98s] - Train Loss: 50.66295, Train Acc: 0.85965Train classification Loss: 0.33635, Train Guidance Loss: 50.32660, Train Penalization Term Loss: -0.93977, Train Reward Term Loss: -503265.04130 | Val Loss: 50.77960, Val Acc: 0.81333Val classification Loss: 0.42275, Val Guidance Loss: 50.35684, Val Penalization Term Loss: -0.93986, Val Reward Term Loss: -503567.51500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 46/100 [05:49<06:49,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 46/100 [6.87s] - Train Loss: 50.65786, Train Acc: 0.86355Train classification Loss: 0.34444, Train Guidance Loss: 50.31342, Train Penalization Term Loss: -0.93956, Train Reward Term Loss: -503133.27309 | Val Loss: 50.75198, Val Acc: 0.82000Val classification Loss: 0.41485, Val Guidance Loss: 50.33714, Val Penalization Term Loss: -0.93956, Val Reward Term Loss: -503370.44458 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 47/100 [05:56<06:27,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 47/100 [6.67s] - Train Loss: 50.60475, Train Acc: 0.88109Train classification Loss: 0.31196, Train Guidance Loss: 50.29280, Train Penalization Term Loss: -0.93934, Train Reward Term Loss: -502927.02906 | Val Loss: 50.74299, Val Acc: 0.82000Val classification Loss: 0.41763, Val Guidance Loss: 50.32537, Val Penalization Term Loss: -0.93942, Val Reward Term Loss: -503252.73458 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 48/100 [06:03<06:18,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 48/100 [7.18s] - Train Loss: 50.61145, Train Acc: 0.85965Train classification Loss: 0.31506, Train Guidance Loss: 50.29639, Train Penalization Term Loss: -0.93922, Train Reward Term Loss: -502962.97070 | Val Loss: 50.82161, Val Acc: 0.77333Val classification Loss: 0.47004, Val Guidance Loss: 50.35158, Val Penalization Term Loss: -0.93960, Val Reward Term Loss: -503514.83708 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 49/100 [06:14<06:59,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 49/100 [10.45s] - Train Loss: 50.52793, Train Acc: 0.90058Train classification Loss: 0.25301, Train Guidance Loss: 50.27492, Train Penalization Term Loss: -0.93882, Train Reward Term Loss: -502748.30093 | Val Loss: 50.76596, Val Acc: 0.80667Val classification Loss: 0.42729, Val Guidance Loss: 50.33867, Val Penalization Term Loss: -0.93940, Val Reward Term Loss: -503385.73125 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 50/100 [06:20<06:31,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 50/100 [6.92s] - Train Loss: 50.55289, Train Acc: 0.87719Train classification Loss: 0.28437, Train Guidance Loss: 50.26852, Train Penalization Term Loss: -0.93867, Train Reward Term Loss: -502684.26255 | Val Loss: 50.73820, Val Acc: 0.80667Val classification Loss: 0.43188, Val Guidance Loss: 50.30633, Val Penalization Term Loss: -0.93890, Val Reward Term Loss: -503062.34417 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 51/100 [06:27<06:10,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 51/100 [6.95s] - Train Loss: 50.52670, Train Acc: 0.90058Train classification Loss: 0.27147, Train Guidance Loss: 50.25523, Train Penalization Term Loss: -0.93839, Train Reward Term Loss: -502551.33352 | Val Loss: 50.70366, Val Acc: 0.82000Val classification Loss: 0.41622, Val Guidance Loss: 50.28744, Val Penalization Term Loss: -0.93848, Val Reward Term Loss: -502873.48583 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 52/100 [06:34<05:54,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 52/100 [6.95s] - Train Loss: 50.54712, Train Acc: 0.88304Train classification Loss: 0.31126, Train Guidance Loss: 50.23585, Train Penalization Term Loss: -0.93817, Train Reward Term Loss: -502357.58760 | Val Loss: 50.70149, Val Acc: 0.81333Val classification Loss: 0.42217, Val Guidance Loss: 50.27932, Val Penalization Term Loss: -0.93852, Val Reward Term Loss: -502792.28583 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 53/100 [06:41<05:40,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 53/100 [6.90s] - Train Loss: 50.53140, Train Acc: 0.87914Train classification Loss: 0.28325, Train Guidance Loss: 50.24814, Train Penalization Term Loss: -0.93827, Train Reward Term Loss: -502480.51925 | Val Loss: 50.71105, Val Acc: 0.82667Val classification Loss: 0.41621, Val Guidance Loss: 50.29484, Val Penalization Term Loss: -0.93875, Val Reward Term Loss: -502947.43250 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 54/100 [06:52<06:20,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 54/100 [10.68s] - Train Loss: 50.51149, Train Acc: 0.88304Train classification Loss: 0.28173, Train Guidance Loss: 50.22976, Train Penalization Term Loss: -0.93799, Train Reward Term Loss: -502296.67099 | Val Loss: 50.71253, Val Acc: 0.80667Val classification Loss: 0.42696, Val Guidance Loss: 50.28556, Val Penalization Term Loss: -0.93858, Val Reward Term Loss: -502854.70292 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 55/100 [06:58<05:47,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 55/100 [6.46s] - Train Loss: 50.47416, Train Acc: 0.92203Train classification Loss: 0.24131, Train Guidance Loss: 50.23285, Train Penalization Term Loss: -0.93815, Train Reward Term Loss: -502327.58547 | Val Loss: 50.73379, Val Acc: 0.81333Val classification Loss: 0.43990, Val Guidance Loss: 50.29388, Val Penalization Term Loss: -0.93872, Val Reward Term Loss: -502937.92250 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 56/100 [07:05<05:23,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 56/100 [6.47s] - Train Loss: 50.44376, Train Acc: 0.93177Train classification Loss: 0.22348, Train Guidance Loss: 50.22028, Train Penalization Term Loss: -0.93782, Train Reward Term Loss: -502201.87926 | Val Loss: 50.70099, Val Acc: 0.82667Val classification Loss: 0.42693, Val Guidance Loss: 50.27405, Val Penalization Term Loss: -0.93854, Val Reward Term Loss: -502739.60000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 57/100 [07:11<05:05,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 57/100 [6.50s] - Train Loss: 50.48496, Train Acc: 0.89864Train classification Loss: 0.25726, Train Guidance Loss: 50.22770, Train Penalization Term Loss: -0.93793, Train Reward Term Loss: -502276.09954 | Val Loss: 50.69155, Val Acc: 0.82000Val classification Loss: 0.43135, Val Guidance Loss: 50.26019, Val Penalization Term Loss: -0.93815, Val Reward Term Loss: -502600.98958 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 58/100 [07:18<04:50,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 58/100 [6.51s] - Train Loss: 50.46015, Train Acc: 0.90253Train classification Loss: 0.25179, Train Guidance Loss: 50.20836, Train Penalization Term Loss: -0.93751, Train Reward Term Loss: -502082.69347 | Val Loss: 50.66046, Val Acc: 0.82667Val classification Loss: 0.41905, Val Guidance Loss: 50.24141, Val Penalization Term Loss: -0.93774, Val Reward Term Loss: -502413.15708 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 59/100 [07:28<05:22,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 59/100 [10.08s] - Train Loss: 50.46810, Train Acc: 0.90448Train classification Loss: 0.26507, Train Guidance Loss: 50.20303, Train Penalization Term Loss: -0.93751, Train Reward Term Loss: -502029.37634 | Val Loss: 50.74810, Val Acc: 0.80667Val classification Loss: 0.48999, Val Guidance Loss: 50.25812, Val Penalization Term Loss: -0.93799, Val Reward Term Loss: -502580.25333 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 60/100 [07:34<04:57,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 60/100 [6.39s] - Train Loss: 50.38442, Train Acc: 0.92398Train classification Loss: 0.19978, Train Guidance Loss: 50.18465, Train Penalization Term Loss: -0.93718, Train Reward Term Loss: -501845.56092 | Val Loss: 50.64961, Val Acc: 0.82667Val classification Loss: 0.42659, Val Guidance Loss: 50.22302, Val Penalization Term Loss: -0.93745, Val Reward Term Loss: -502229.31333 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 61/100 [07:41<04:37,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 61/100 [6.42s] - Train Loss: 50.40169, Train Acc: 0.92008Train classification Loss: 0.22175, Train Guidance Loss: 50.17994, Train Penalization Term Loss: -0.93702, Train Reward Term Loss: -501798.46729 | Val Loss: 50.71884, Val Acc: 0.80000Val classification Loss: 0.46348, Val Guidance Loss: 50.25536, Val Penalization Term Loss: -0.93786, Val Reward Term Loss: -502552.67083 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 62/100 [07:47<04:22,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 62/100 [6.38s] - Train Loss: 50.34728, Train Acc: 0.94347Train classification Loss: 0.15781, Train Guidance Loss: 50.18948, Train Penalization Term Loss: -0.93718, Train Reward Term Loss: -501893.84795 | Val Loss: 50.68141, Val Acc: 0.81333Val classification Loss: 0.45863, Val Guidance Loss: 50.22279, Val Penalization Term Loss: -0.93752, Val Reward Term Loss: -502226.95917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 63/100 [07:54<04:11,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 63/100 [6.53s] - Train Loss: 50.37434, Train Acc: 0.92788Train classification Loss: 0.20580, Train Guidance Loss: 50.16854, Train Penalization Term Loss: -0.93678, Train Reward Term Loss: -501684.42629 | Val Loss: 50.73916, Val Acc: 0.80667Val classification Loss: 0.49546, Val Guidance Loss: 50.24370, Val Penalization Term Loss: -0.93758, Val Reward Term Loss: -502436.07292 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 64/100 [08:04<04:40,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 64/100 [10.15s] - Train Loss: 50.37642, Train Acc: 0.91618Train classification Loss: 0.21396, Train Guidance Loss: 50.16246, Train Penalization Term Loss: -0.93657, Train Reward Term Loss: -501623.66222 | Val Loss: 50.64916, Val Acc: 0.84000Val classification Loss: 0.43935, Val Guidance Loss: 50.20980, Val Penalization Term Loss: -0.93720, Val Reward Term Loss: -502097.11083 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 65/100 [08:10<04:19,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 65/100 [6.50s] - Train Loss: 50.36053, Train Acc: 0.92398Train classification Loss: 0.20314, Train Guidance Loss: 50.15739, Train Penalization Term Loss: -0.93655, Train Reward Term Loss: -501572.97643 | Val Loss: 50.74393, Val Acc: 0.80000Val classification Loss: 0.52927, Val Guidance Loss: 50.21466, Val Penalization Term Loss: -0.93736, Val Reward Term Loss: -502145.72000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 66/100 [08:17<04:03,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 66/100 [6.56s] - Train Loss: 50.30298, Train Acc: 0.93177Train classification Loss: 0.17082, Train Guidance Loss: 50.13216, Train Penalization Term Loss: -0.93600, Train Reward Term Loss: -501320.69420 | Val Loss: 50.71239, Val Acc: 0.79333Val classification Loss: 0.50961, Val Guidance Loss: 50.20278, Val Penalization Term Loss: -0.93701, Val Reward Term Loss: -502026.87917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 67/100 [08:23<03:49,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 67/100 [6.52s] - Train Loss: 50.28014, Train Acc: 0.94542Train classification Loss: 0.14092, Train Guidance Loss: 50.13923, Train Penalization Term Loss: -0.93627, Train Reward Term Loss: -501391.35581 | Val Loss: 50.66413, Val Acc: 0.81333Val classification Loss: 0.47129, Val Guidance Loss: 50.19284, Val Penalization Term Loss: -0.93700, Val Reward Term Loss: -501927.48083 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 68/100 [08:30<03:38,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 68/100 [6.51s] - Train Loss: 50.30616, Train Acc: 0.92788Train classification Loss: 0.18893, Train Guidance Loss: 50.11723, Train Penalization Term Loss: -0.93577, Train Reward Term Loss: -501171.41825 | Val Loss: 50.59985, Val Acc: 0.84667Val classification Loss: 0.43986, Val Guidance Loss: 50.15998, Val Penalization Term Loss: -0.93620, Val Reward Term Loss: -501598.90958 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 69/100 [08:40<04:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 69/100 [9.96s] - Train Loss: 50.29555, Train Acc: 0.92203Train classification Loss: 0.18543, Train Guidance Loss: 50.11012, Train Penalization Term Loss: -0.93564, Train Reward Term Loss: -501100.26462 | Val Loss: 50.59251, Val Acc: 0.82667Val classification Loss: 0.43481, Val Guidance Loss: 50.15770, Val Penalization Term Loss: -0.93627, Val Reward Term Loss: -501576.10292 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 70/100 [08:46<03:40,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 70/100 [6.41s] - Train Loss: 50.28986, Train Acc: 0.92008Train classification Loss: 0.18707, Train Guidance Loss: 50.10279, Train Penalization Term Loss: -0.93552, Train Reward Term Loss: -501026.93860 | Val Loss: 50.60379, Val Acc: 0.80667Val classification Loss: 0.47389, Val Guidance Loss: 50.12990, Val Penalization Term Loss: -0.93575, Val Reward Term Loss: -501298.04583 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 71/100 [08:53<03:24,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 71/100 [6.36s] - Train Loss: 50.22746, Train Acc: 0.95322Train classification Loss: 0.13219, Train Guidance Loss: 50.09527, Train Penalization Term Loss: -0.93537, Train Reward Term Loss: -500951.73575 | Val Loss: 50.62689, Val Acc: 0.79333Val classification Loss: 0.48319, Val Guidance Loss: 50.14370, Val Penalization Term Loss: -0.93598, Val Reward Term Loss: -501436.07000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 72/100 [08:59<03:11,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 72/100 [6.37s] - Train Loss: 50.24660, Train Acc: 0.94347Train classification Loss: 0.15601, Train Guidance Loss: 50.09059, Train Penalization Term Loss: -0.93526, Train Reward Term Loss: -500904.93324 | Val Loss: 50.69939, Val Acc: 0.78000Val classification Loss: 0.55672, Val Guidance Loss: 50.14268, Val Penalization Term Loss: -0.93585, Val Reward Term Loss: -501425.84583 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 73/100 [09:05<03:01,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 73/100 [6.43s] - Train Loss: 50.25722, Train Acc: 0.94152Train classification Loss: 0.17389, Train Guidance Loss: 50.08333, Train Penalization Term Loss: -0.93513, Train Reward Term Loss: -500832.41374 | Val Loss: 50.68294, Val Acc: 0.79333Val classification Loss: 0.53123, Val Guidance Loss: 50.15171, Val Penalization Term Loss: -0.93601, Val Reward Term Loss: -501516.16667 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 74/100 [09:16<03:21,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 74/100 [10.07s] - Train Loss: 50.23617, Train Acc: 0.95322Train classification Loss: 0.14465, Train Guidance Loss: 50.09151, Train Penalization Term Loss: -0.93531, Train Reward Term Loss: -500914.20973 | Val Loss: 50.57187, Val Acc: 0.82000Val classification Loss: 0.44938, Val Guidance Loss: 50.12248, Val Penalization Term Loss: -0.93571, Val Reward Term Loss: -501223.92167 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 75/100 [09:22<03:03,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 75/100 [6.37s] - Train Loss: 50.22259, Train Acc: 0.94737Train classification Loss: 0.12556, Train Guidance Loss: 50.09703, Train Penalization Term Loss: -0.93532, Train Reward Term Loss: -500969.33443 | Val Loss: 50.58721, Val Acc: 0.84000Val classification Loss: 0.45991, Val Guidance Loss: 50.12730, Val Penalization Term Loss: -0.93556, Val Reward Term Loss: -501272.10917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 76/100 [09:28<02:49,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 76/100 [6.44s] - Train Loss: 50.22445, Train Acc: 0.96101Train classification Loss: 0.13276, Train Guidance Loss: 50.09169, Train Penalization Term Loss: -0.93523, Train Reward Term Loss: -500915.99062 | Val Loss: 50.68629, Val Acc: 0.79333Val classification Loss: 0.53758, Val Guidance Loss: 50.14871, Val Penalization Term Loss: -0.93611, Val Reward Term Loss: -501486.19167 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 77/100 [09:35<02:37,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 77/100 [6.40s] - Train Loss: 50.21455, Train Acc: 0.96101Train classification Loss: 0.12971, Train Guidance Loss: 50.08484, Train Penalization Term Loss: -0.93502, Train Reward Term Loss: -500847.47996 | Val Loss: 50.69518, Val Acc: 0.79333Val classification Loss: 0.55096, Val Guidance Loss: 50.14422, Val Penalization Term Loss: -0.93601, Val Reward Term Loss: -501441.27750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 78/100 [09:41<02:27,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 78/100 [6.40s] - Train Loss: 50.25432, Train Acc: 0.92398Train classification Loss: 0.16831, Train Guidance Loss: 50.08601, Train Penalization Term Loss: -0.93510, Train Reward Term Loss: -500859.22368 | Val Loss: 50.61741, Val Acc: 0.82667Val classification Loss: 0.49182, Val Guidance Loss: 50.12559, Val Penalization Term Loss: -0.93556, Val Reward Term Loss: -501255.00333 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 79/100 [09:51<02:41,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 79/100 [9.97s] - Train Loss: 50.20640, Train Acc: 0.94347Train classification Loss: 0.12896, Train Guidance Loss: 50.07744, Train Penalization Term Loss: -0.93505, Train Reward Term Loss: -500773.49239 | Val Loss: 50.58966, Val Acc: 0.82000Val classification Loss: 0.48032, Val Guidance Loss: 50.10934, Val Penalization Term Loss: -0.93536, Val Reward Term Loss: -501092.50500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 80/100 [09:58<02:26,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 80/100 [6.46s] - Train Loss: 50.18624, Train Acc: 0.97076Train classification Loss: 0.10809, Train Guidance Loss: 50.07815, Train Penalization Term Loss: -0.93507, Train Reward Term Loss: -500780.55702 | Val Loss: 50.78951, Val Acc: 0.78667Val classification Loss: 0.67292, Val Guidance Loss: 50.11659, Val Penalization Term Loss: -0.93553, Val Reward Term Loss: -501164.93250 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 81/100 [10:04<02:13,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 81/100 [6.36s] - Train Loss: 50.17914, Train Acc: 0.96881Train classification Loss: 0.09763, Train Guidance Loss: 50.08152, Train Penalization Term Loss: -0.93515, Train Reward Term Loss: -500814.24391 | Val Loss: 50.80290, Val Acc: 0.77333Val classification Loss: 0.65479, Val Guidance Loss: 50.14812, Val Penalization Term Loss: -0.93596, Val Reward Term Loss: -501480.23917 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 82/100 [10:10<02:03,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 82/100 [6.42s] - Train Loss: 50.17233, Train Acc: 0.97076Train classification Loss: 0.08981, Train Guidance Loss: 50.08252, Train Penalization Term Loss: -0.93499, Train Reward Term Loss: -500824.26249 | Val Loss: 50.75243, Val Acc: 0.80000Val classification Loss: 0.61744, Val Guidance Loss: 50.13499, Val Penalization Term Loss: -0.93568, Val Reward Term Loss: -501348.98458 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 83/100 [10:17<01:54,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 83/100 [6.44s] - Train Loss: 50.22202, Train Acc: 0.94542Train classification Loss: 0.15129, Train Guidance Loss: 50.07073, Train Penalization Term Loss: -0.93483, Train Reward Term Loss: -500706.41831 | Val Loss: 50.81360, Val Acc: 0.78000Val classification Loss: 0.68894, Val Guidance Loss: 50.12467, Val Penalization Term Loss: -0.93559, Val Reward Term Loss: -501245.74417 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 84/100 [10:27<02:03,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 84/100 [9.93s] - Train Loss: 50.15244, Train Acc: 0.97076Train classification Loss: 0.08955, Train Guidance Loss: 50.06289, Train Penalization Term Loss: -0.93474, Train Reward Term Loss: -500627.93969 | Val Loss: 50.99036, Val Acc: 0.74000Val classification Loss: 0.85238, Val Guidance Loss: 50.13798, Val Penalization Term Loss: -0.93578, Val Reward Term Loss: -501378.85500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 85/100 [10:33<01:49,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 85/100 [6.37s] - Train Loss: 50.16011, Train Acc: 0.96101Train classification Loss: 0.10412, Train Guidance Loss: 50.05599, Train Penalization Term Loss: -0.93454, Train Reward Term Loss: -500558.95486 | Val Loss: 50.63044, Val Acc: 0.80000Val classification Loss: 0.52511, Val Guidance Loss: 50.10532, Val Penalization Term Loss: -0.93538, Val Reward Term Loss: -501052.31750 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 86/100 [10:40<01:38,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 86/100 [6.38s] - Train Loss: 50.14067, Train Acc: 0.96296Train classification Loss: 0.09777, Train Guidance Loss: 50.04290, Train Penalization Term Loss: -0.93436, Train Reward Term Loss: -500428.08260 | Val Loss: 50.73538, Val Acc: 0.79333Val classification Loss: 0.63193, Val Guidance Loss: 50.10346, Val Penalization Term Loss: -0.93524, Val Reward Term Loss: -501033.67083 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 87/100 [10:46<01:28,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 87/100 [6.41s] - Train Loss: 50.16806, Train Acc: 0.96296Train classification Loss: 0.11561, Train Guidance Loss: 50.05245, Train Penalization Term Loss: -0.93443, Train Reward Term Loss: -500523.62317 | Val Loss: 50.62130, Val Acc: 0.82000Val classification Loss: 0.53390, Val Guidance Loss: 50.08741, Val Penalization Term Loss: -0.93490, Val Reward Term Loss: -500873.14000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 88/100 [10:52<01:20,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 88/100 [6.38s] - Train Loss: 50.15436, Train Acc: 0.96296Train classification Loss: 0.09508, Train Guidance Loss: 50.05927, Train Penalization Term Loss: -0.93451, Train Reward Term Loss: -500591.79459 | Val Loss: 50.61821, Val Acc: 0.80667Val classification Loss: 0.53986, Val Guidance Loss: 50.07835, Val Penalization Term Loss: -0.93473, Val Reward Term Loss: -500782.53125 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 89/100 [11:02<01:24,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 89/100 [9.99s] - Train Loss: 50.11590, Train Acc: 0.98051Train classification Loss: 0.07354, Train Guidance Loss: 50.04236, Train Penalization Term Loss: -0.93432, Train Reward Term Loss: -500422.70066 | Val Loss: 50.62451, Val Acc: 0.82667Val classification Loss: 0.53599, Val Guidance Loss: 50.08852, Val Penalization Term Loss: -0.93486, Val Reward Term Loss: -500884.25875 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 90/100 [11:09<01:13,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 90/100 [6.40s] - Train Loss: 50.10605, Train Acc: 0.98246Train classification Loss: 0.05819, Train Guidance Loss: 50.04786, Train Penalization Term Loss: -0.93433, Train Reward Term Loss: -500477.70236 | Val Loss: 50.72549, Val Acc: 0.80000Val classification Loss: 0.61206, Val Guidance Loss: 50.11343, Val Penalization Term Loss: -0.93528, Val Reward Term Loss: -501133.40958 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 91/100 [11:15<01:03,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 91/100 [6.45s] - Train Loss: 50.15581, Train Acc: 0.96296Train classification Loss: 0.10587, Train Guidance Loss: 50.04994, Train Penalization Term Loss: -0.93431, Train Reward Term Loss: -500498.51072 | Val Loss: 50.62722, Val Acc: 0.83333Val classification Loss: 0.54738, Val Guidance Loss: 50.07984, Val Penalization Term Loss: -0.93475, Val Reward Term Loss: -500797.48083 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 92/100 [11:22<00:54,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 92/100 [6.40s] - Train Loss: 50.16231, Train Acc: 0.95322Train classification Loss: 0.11592, Train Guidance Loss: 50.04639, Train Penalization Term Loss: -0.93408, Train Reward Term Loss: -500462.93250 | Val Loss: 50.66062, Val Acc: 0.80667Val classification Loss: 0.56017, Val Guidance Loss: 50.10046, Val Penalization Term Loss: -0.93508, Val Reward Term Loss: -501003.64417 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 93/100 [11:28<00:46,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 93/100 [6.36s] - Train Loss: 50.11775, Train Acc: 0.97466Train classification Loss: 0.07074, Train Guidance Loss: 50.04701, Train Penalization Term Loss: -0.93421, Train Reward Term Loss: -500469.17410 | Val Loss: 50.74251, Val Acc: 0.78667Val classification Loss: 0.64296, Val Guidance Loss: 50.09955, Val Penalization Term Loss: -0.93519, Val Reward Term Loss: -500994.60500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 93/100 [11:38<00:52,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 94/100 [9.94s] - Train Loss: 50.13908, Train Acc: 0.96101Train classification Loss: 0.10235, Train Guidance Loss: 50.03673, Train Penalization Term Loss: -0.93414, Train Reward Term Loss: -500366.38700 | Val Loss: 50.65905, Val Acc: 0.80000Val classification Loss: 0.60533, Val Guidance Loss: 50.05372, Val Penalization Term Loss: -0.93441, Val Reward Term Loss: -500536.32083 | \n",
      "\n",
      "Early stopping triggered. Restoring best model weights from epoch 74.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 1 - Best Epoch 74, Val Loss: 50.5719, Acc: 0.8200, AUC: 0.9071, \n",
      "    Classification Loss: 0.4494, Guidance Loss: 50.1225, Penalization Term Loss: -0.9357, Reward Term Loss: -501223.9217\n",
      "    Sensitivity: 0.8493, Specificity: 0.7922, BACC: 0.8208, F1: 0.8200, MCC: 0.6418\n",
      "    Fold 1 finished in 699.60 seconds.\n",
      "    Test Loss: 51.0400, Test Accuracy: 0.7697, AUC: 0.7637, \n",
      "   Classification Loss: 0.9823, Guidance Loss: 50.0577, Penalization Term Loss: -0.9343, Reward Term Loss: -500575.7814\n",
      "    Sensitivity: 0.6538, Specificity: 0.8736, BACC: 0.7637, F1: 0.7662, MCC: 0.5437\n",
      "Combination finished in 700.61 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Base directory for layers and heads\n",
    "if USE_MODEL == \"hybrid\":\n",
    "    learning_rate = 2e-7\n",
    "    weight_decay = 4e-3\n",
    "    CLS_LAYER = 6\n",
    "    NUM_EPOCHS = 100\n",
    "    EARLY_STOP_PATIENCE = 20\n",
    "    RESULTS_BASE_DIR = \"/home/diogommiranda/tese/outputs/roi_guidance/resnetvit/\" + DATASET\n",
    "elif USE_MODEL == \"purevit\":\n",
    "    learning_rate = 5e-7\n",
    "    weight_decay = 4e-3\n",
    "    CLS_LAYER = 12\n",
    "    NUM_EPOCHS = 1000\n",
    "    EARLY_STOP_PATIENCE = NUM_EPOCHS\n",
    "    RESULTS_BASE_DIR = \"/home/diogommiranda/tese/outputs/roi_guidance/purevit/\" + DATASET\n",
    "else:\n",
    "    raise ValueError(\"Invalid model type. Choose 'hybrid' or 'purevit'.\")\n",
    "os.makedirs(RESULTS_BASE_DIR, exist_ok=True)\n",
    "\n",
    "lambda_factors = [1e-4]\n",
    "#T = 0.01\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Set up StratifiedGroupKFold by subjects\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "subject_ids_list = [extract_subject_id(p) for p in train_paths]\n",
    "subject_ids = np.array(subject_ids_list)\n",
    "\n",
    "overall_start_time = time.time()\n",
    "            \n",
    "# For-loop over determined learning rates and weight decays\n",
    "\n",
    "for T in [0.1]: \n",
    "    for lambda_factor in lambda_factors:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Testing Combination: lambda={lambda_factor} for CLS_LAYER={CLS_LAYER} and {USE_MODEL} model\")\n",
    "\n",
    "        combo_dir_name = f\"Layer_{CLS_LAYER}/T={T}/lambda_{lambda_factor}\"\n",
    "        combo_results_dir = os.path.join(RESULTS_BASE_DIR, combo_dir_name)\n",
    "        if os.path.exists(combo_results_dir):\n",
    "            print(f\"\\nDirectory {combo_results_dir} already exists. Skipping...\\n\")\n",
    "            continue\n",
    "        os.makedirs(combo_results_dir, exist_ok=True)\n",
    "        print(f\"    Saving results for this combo to: {combo_results_dir}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        start_time_combination = time.time()\n",
    "        \n",
    "        current_combo_val_losses = []\n",
    "        \n",
    "        current_combo_val_accuracies = []\n",
    "        current_combo_val_aucs = []\n",
    "        current_combo_best_epoch = []\n",
    "        \n",
    "        current_combo_val_sensitivities = []\n",
    "        current_combo_val_specificities = []\n",
    "        current_combo_val_baccs = []\n",
    "        current_combo_val_f1s = []\n",
    "        current_combo_val_mccs = []\n",
    "\n",
    "        current_combo_test_losses = []\n",
    "\n",
    "        \n",
    "        current_combo_test_accuracies = []\n",
    "        current_combo_test_aucs = []\n",
    "        \n",
    "        current_combo_test_sensitivities = []\n",
    "        current_combo_test_specificities = []\n",
    "        current_combo_test_baccs = []\n",
    "        current_combo_test_f1s = []\n",
    "        current_combo_test_mccs = []\n",
    "\n",
    "        fold_no = 1\n",
    "\n",
    "        # K-Fold Cross-Validation\n",
    "        for train_indices, val_indices in sgkf.split(train_paths, train_labels, groups=subject_ids):\n",
    "            \n",
    "            # Runs only desired fold and skips the rest: for testing purposes\n",
    "            if fold_no > 1:\n",
    "                continue\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Training Fold {fold_no}/{N_SPLITS}...\")\n",
    "            start_time_fold = time.time()\n",
    "\n",
    "            # Get paths and labels for the current fold\n",
    "            fold_train_paths = train_paths[train_indices]\n",
    "            fold_train_labels = train_labels[train_indices]\n",
    "            fold_val_paths = train_paths[val_indices]\n",
    "            fold_val_labels = train_labels[val_indices]\n",
    "\n",
    "            # Verify subject separation\n",
    "            train_subjects = set(subject_ids[train_indices])\n",
    "            val_subjects = set(subject_ids[val_indices])\n",
    "            if not train_subjects.isdisjoint(val_subjects):\n",
    "                    raise RuntimeError(f\"WARNING: Fold {fold_no} has overlapping subjects!\")\n",
    "\n",
    "            print(f\"  Train samples: {len(fold_train_paths)}, Validation samples: {len(fold_val_paths)}\")\n",
    "\n",
    "            # Calculate minmax parameters for the current training fold\n",
    "            fold_min, fold_max = calculate_min_max(fold_train_paths)\n",
    "\n",
    "            # Create fold train dataset\n",
    "            fold_train_data = create_dataloader(\n",
    "                paths=fold_train_paths,\n",
    "                labels=fold_train_labels,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                volume_shape=VOLUME_SHAPE,\n",
    "                is_training=True, \n",
    "                seed=seed,\n",
    "                min_val=fold_min, \n",
    "                max_val=fold_max,\n",
    "                apply_padding=apply_padding\n",
    "            ) \n",
    "            \n",
    "            # Create fold validation dataset\n",
    "            fold_val_data = create_dataloader(\n",
    "                paths=fold_val_paths,\n",
    "                labels=fold_val_labels,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                volume_shape=VOLUME_SHAPE,\n",
    "                is_training=False, \n",
    "                seed=None,\n",
    "                min_val=fold_min, \n",
    "                max_val=fold_max,\n",
    "                apply_padding=apply_padding\n",
    "            )\n",
    "            \n",
    "            test_data = create_dataloader(\n",
    "                paths=test_paths,\n",
    "                labels=test_labels,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                volume_shape=VOLUME_SHAPE,\n",
    "                is_training=False,\n",
    "                seed=None,\n",
    "                min_val=fold_min, \n",
    "                max_val=fold_max,\n",
    "                apply_padding=apply_padding\n",
    "                )\n",
    "\n",
    "            if fold_train_data is None or fold_val_data is None or test_data is None:\n",
    "                    raise RuntimeError(f\"ERROR: Could not create datasets for fold {fold_no}.\")\n",
    "                \n",
    "            # Compute class weights for the current fold\n",
    "            unique_classes, class_counts = np.unique(fold_train_labels, return_counts=True)\n",
    "            print(f\"  Fold train label counts: {dict(zip(unique_classes, class_counts))}\")\n",
    "            pos_weight_val = class_counts[0] / class_counts[1]\n",
    "            pos_weight_val = torch.tensor([pos_weight_val], device=device)\n",
    "            print(f\"  Calculated pos_weight for BCEWithLogitsLoss: {pos_weight_val.item():.4f}\")\n",
    "\n",
    "            # Create the model\n",
    "            if USE_MODEL == \"hybrid\":\n",
    "                resnet_feature_extractor = ResnetFeatureExtractor(resnet_config=resnet_config)\n",
    "                model = ResNetViT(resnet_feature_extractor, vit_config).to(device)\n",
    "            elif USE_MODEL == \"purevit\":\n",
    "                model = pureViT(purevit_config).to(device)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid model type. Choose 'hybrid' or 'purevit'.\")\n",
    "\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_val).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "            best_val_loss = float('inf')\n",
    "            best_val_bacc = -float('inf')\n",
    "            epochs_no_improve = 0\n",
    "            best_epoch = 0\n",
    "            best_model_state = None\n",
    "            stopped_epoch = NUM_EPOCHS \n",
    "\n",
    "            # Initialize history for this fold\n",
    "            history = {'epoch': [], 'train_loss': [], 'train_accuracy': [], 'train_auc': [],\n",
    "                    'train_classification_loss': [], 'train_guidance_loss': [], 'train_penalization_term_loss': [], 'train_reward_term_loss':[], 'train_avg_att_roi': [], 'train_avg_att_non_roi': [],\n",
    "                    'val_loss': [], 'val_accuracy': [], 'val_auc': [],\n",
    "                    'val_classification_loss': [], 'val_guidance_loss': [], 'val_penalization_term_loss': [], 'val_reward_term_loss': [], 'val_avg_att_roi': [], 'val_avg_att_non_roi': [],\n",
    "                    'val_sensitivity': [], 'val_specificity': [], 'val_bacc': [], 'val_f1': [], 'val_mcc': []}\n",
    "\n",
    "            for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "                epoch_start_time = time.time()\n",
    "                history['epoch'].append(epoch + 1)\n",
    "\n",
    "                # ---------------- Train phase ----------------\n",
    "                model.train()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                \n",
    "                running_classification_loss = 0.0\n",
    "                running_guidance_loss = 0.0\n",
    "                running_penalization_term_loss = 0.0\n",
    "                running_reward_term_loss = 0.0\n",
    "                \n",
    "                running_avg_att_roi = 0.0\n",
    "                running_avg_att_non_roi = 0.0\n",
    "                \n",
    "                train_pred_probs = []\n",
    "                train_targets = []\n",
    "                \n",
    "                for i, (inputs, labels) in enumerate(fold_train_data):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.float().unsqueeze(1).to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Mixed precision\n",
    "                    with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=USE_AMP):\n",
    "                        outputs, attention_maps = model(inputs, return_attention=RETURN_ATTENTION)\n",
    "                        classification_loss = criterion(outputs, labels)\n",
    "                    \n",
    "                        if USE_MODEL == \"hybrid\":\n",
    "                            attn_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True)\n",
    "                        elif USE_MODEL == \"purevit\":\n",
    "                            attn_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True) # determine the exact layer later   \n",
    "                        cls_attn = attn_map[:, 0, 1:] # Get CLS attention vector\n",
    "                        \n",
    "                        volume_att = upsample_vit_attention(cls_attn, EXPECTED_FINAL_VOLUME_SHAPE, (6, 7, 6)) # Get voxel-wise CLS attention map\n",
    "                        volume_att = volume_att.flatten(start_dim=1) # first dimension is the batch size, which is 1\n",
    "                        volume_att = F.softmax(volume_att / T, dim=-1)  # Apply softmax to the CLS attention vector\n",
    "                        volume_att = volume_att.reshape(len(inputs), EXPECTED_FINAL_VOLUME_SHAPE[0], EXPECTED_FINAL_VOLUME_SHAPE[1], EXPECTED_FINAL_VOLUME_SHAPE[2])  # Reshape back to the original volume shape\n",
    "\n",
    "                        penalization_term_loss = torch.mean(torch.sum(torch.multiply(torch.log(1-volume_att+1e-10), 1-mask_volume), dim=(1,2,3))) # penalization term of the guidance loss across the batch\n",
    "                        reward_term_loss = torch.mean(torch.sum(torch.multiply(torch.log(volume_att+1e-10), mask_volume), dim=(1,2,3))) # reward term of the guidance loss across the batch\n",
    "                        guidance_loss = - lambda_factor * (penalization_term_loss + reward_term_loss)\n",
    "                    \n",
    "                        loss = classification_loss + guidance_loss\n",
    "                        \n",
    "                        avg_att_roi = torch.mean(torch.sum(torch.multiply(volume_att, mask_volume), dim=(1,2,3)) / roi_voxels)\n",
    "                        avg_att_non_roi = torch.mean(torch.sum(torch.multiply(volume_att, (1 - mask_volume)), dim=(1,2,3)) / non_roi_voxels)           \n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    running_classification_loss += classification_loss.item() * inputs.size(0)\n",
    "                    running_guidance_loss += guidance_loss.item() * inputs.size(0)\n",
    "                    running_penalization_term_loss += penalization_term_loss.item() * inputs.size(0)\n",
    "                    running_reward_term_loss += reward_term_loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    running_avg_att_roi = avg_att_roi.item() * inputs.size(0)\n",
    "                    running_avg_att_non_roi = avg_att_non_roi.item() * inputs.size(0)\n",
    "                    \n",
    "                    prob_outputs = torch.sigmoid(outputs)\n",
    "                    train_pred_probs.extend(prob_outputs.detach().cpu().numpy())\n",
    "                    train_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "                train_targets = np.array(train_targets).flatten()\n",
    "                train_pred_probs = np.array(train_pred_probs).flatten()\n",
    "                \n",
    "                # Calculate training metrics for the epoch\n",
    "                train_loss = running_loss / len(fold_train_data.dataset)\n",
    "                \n",
    "                train_classification_loss = running_classification_loss / len(fold_train_data.dataset)\n",
    "                train_guidance_loss = running_guidance_loss / len(fold_train_data.dataset)\n",
    "                train_penalization_term_loss = running_penalization_term_loss / len(fold_train_data.dataset)\n",
    "                train_reward_term_loss = running_reward_term_loss / len(fold_train_data.dataset)\n",
    "                \n",
    "                train_avg_att_roi = running_avg_att_roi / len(fold_train_data.dataset)\n",
    "                train_avg_att_non_roi = running_avg_att_non_roi / len(fold_train_data.dataset)\n",
    "                \n",
    "                train_acc = accuracy_score(train_targets, train_pred_probs >= 0.5)\n",
    "                train_auc = roc_auc_score(train_targets, train_pred_probs)\n",
    "                \n",
    "                history['train_loss'].append(train_loss)\n",
    "                \n",
    "                history['train_classification_loss'].append(train_classification_loss)\n",
    "                history['train_guidance_loss'].append(train_guidance_loss)\n",
    "                history['train_penalization_term_loss'].append(train_penalization_term_loss)\n",
    "                history['train_reward_term_loss'].append(train_reward_term_loss)\n",
    "                \n",
    "                history['train_avg_att_roi'].append(train_avg_att_roi)\n",
    "                history['train_avg_att_non_roi'].append(train_avg_att_non_roi)\n",
    "                \n",
    "                history['train_accuracy'].append(train_acc)\n",
    "                history['train_auc'].append(train_auc)\n",
    "\n",
    "                # ---------------- Validation phase ----------------\n",
    "                model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                \n",
    "                running_classification_loss = 0.0\n",
    "                running_guidance_loss = 0.0\n",
    "                running_penalization_term_loss = 0.0\n",
    "                running_reward_term_loss = 0.0\n",
    "                \n",
    "                running_avg_att_roi = 0.0\n",
    "                running_avg_att_non_roi = 0.0\n",
    "                \n",
    "                val_pred_probs = []\n",
    "                val_targets = []\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    for inputs, labels in fold_val_data:\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                        # Mixed precision\n",
    "                        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=USE_AMP):\n",
    "                            outputs, attention_maps = model(inputs, return_attention=RETURN_ATTENTION)\n",
    "                            classification_loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            if USE_MODEL == \"hybrid\":\n",
    "                                attn_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True)\n",
    "                            elif USE_MODEL == \"purevit\":\n",
    "                                attn_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True) # determine the exact layer later   \n",
    "                            cls_attn = attn_map[:, 0, 1:] # CLS attention vector\n",
    "                                                    \n",
    "                            volume_att = upsample_vit_attention(cls_attn, EXPECTED_FINAL_VOLUME_SHAPE, (6, 7, 6)) # Get voxel-wise CLS attention map\n",
    "                        \n",
    "                            volume_att = volume_att.flatten(start_dim=1) # first dimension is the batch size, which is 1\n",
    "                            volume_att = F.softmax(volume_att / T, dim=-1)  # Apply softmax to the CLS attention vector\n",
    "                            volume_att = volume_att.reshape(len(inputs), EXPECTED_FINAL_VOLUME_SHAPE[0], EXPECTED_FINAL_VOLUME_SHAPE[1], EXPECTED_FINAL_VOLUME_SHAPE[2])  # Reshape back to the original volume shape\n",
    "\n",
    "                            penalization_term_loss = torch.mean(torch.sum(torch.multiply(torch.log(1-volume_att+1e-10), 1-mask_volume), dim=(1,2,3))) # penalization term of the guidance loss across the batch\n",
    "                            reward_term_loss = torch.mean(torch.sum(torch.multiply(torch.log(volume_att+1e-10), mask_volume), dim=(1,2,3))) # reward term of the guidance loss across the batch\n",
    "                            guidance_loss = - lambda_factor * (penalization_term_loss + reward_term_loss)\n",
    "                        \n",
    "                            loss = classification_loss + guidance_loss\n",
    "                            \n",
    "                            avg_att_roi = torch.mean(torch.sum(torch.multiply(volume_att, mask_volume), dim=(1,2,3)) / roi_voxels)\n",
    "                            avg_att_non_roi = torch.mean(torch.sum(torch.multiply(volume_att, (1 - mask_volume)), dim=(1,2,3)) / non_roi_voxels)\n",
    "                        \n",
    "                        running_loss += loss.item() * inputs.size(0)\n",
    "                        \n",
    "                        running_classification_loss += classification_loss.item() * inputs.size(0)\n",
    "                        running_guidance_loss += guidance_loss.item() * inputs.size(0)\n",
    "                        running_penalization_term_loss += penalization_term_loss.item() * inputs.size(0)\n",
    "                        running_reward_term_loss += reward_term_loss.item() * inputs.size(0)\n",
    "                        \n",
    "                        running_avg_att_roi += avg_att_roi.item() * inputs.size(0)\n",
    "                        running_avg_att_non_roi += avg_att_non_roi.item() * inputs.size(0)\n",
    "\n",
    "                        prob_outputs = torch.sigmoid(outputs)\n",
    "                        val_pred_probs.extend(prob_outputs.cpu().numpy())\n",
    "                        val_targets.extend(labels.cpu().numpy())\n",
    "                        \n",
    "                val_targets = np.array(val_targets).flatten()\n",
    "                val_pred_probs = np.array(val_pred_probs).flatten()\n",
    "\n",
    "                # Calculate validation metrics for the epoch\n",
    "                val_loss = running_loss / len(fold_val_data.dataset)\n",
    "                \n",
    "                val_classification_loss = running_classification_loss / len(fold_val_data.dataset)\n",
    "                val_guidance_loss = running_guidance_loss / len(fold_val_data.dataset)\n",
    "                val_penalization_term_loss = running_penalization_term_loss / len(fold_val_data.dataset)\n",
    "                val_reward_term_loss = running_reward_term_loss / len(fold_val_data.dataset)\n",
    "                \n",
    "                val_avg_att_roi = running_avg_att_roi / len(fold_val_data.dataset)\n",
    "                val_avg_att_non_roi = running_avg_att_non_roi / len(fold_val_data.dataset)\n",
    "                \n",
    "                val_acc = accuracy_score(val_targets, val_pred_probs >= 0.5)\n",
    "                val_auc = roc_auc_score(val_targets, val_pred_probs)\n",
    "                \n",
    "                val_true = val_targets.astype(int)\n",
    "                val_pred = (val_pred_probs >= 0.5).astype(int)\n",
    "                \n",
    "                val_sensitivity = recall_score(val_true, val_pred, pos_label=1)\n",
    "                val_specificity = recall_score(val_true, val_pred, pos_label=0)\n",
    "                val_bacc = balanced_accuracy_score(val_true, val_pred)\n",
    "                val_f1 = f1_score(val_true, val_pred, average='weighted')\n",
    "                val_mcc = matthews_corrcoef(val_true, val_pred)\n",
    "                \n",
    "                history['val_loss'].append(val_loss)\n",
    "                \n",
    "                history['val_classification_loss'].append(val_classification_loss)\n",
    "                history['val_guidance_loss'].append(val_guidance_loss)\n",
    "                history['val_penalization_term_loss'].append(val_penalization_term_loss)\n",
    "                history['val_reward_term_loss'].append(val_reward_term_loss)\n",
    "                \n",
    "                history['val_avg_att_roi'].append(val_avg_att_roi)\n",
    "                history['val_avg_att_non_roi'].append(val_avg_att_non_roi)\n",
    "                \n",
    "                history['val_accuracy'].append(val_acc)\n",
    "                history['val_auc'].append(val_auc)\n",
    "                \n",
    "                history['val_sensitivity'].append(val_sensitivity)\n",
    "                history['val_specificity'].append(val_specificity)\n",
    "                history['val_bacc'].append(val_bacc)\n",
    "                history['val_f1'].append(val_f1)\n",
    "                history['val_mcc'].append(val_mcc)\n",
    "\n",
    "                # Print Epoch Summary\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                print(f\"    Epoch {epoch+1}/{NUM_EPOCHS} [{epoch_duration:.2f}s] - \"\n",
    "                        f\"Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.5f}\"\n",
    "                        f\"Train classification Loss: {train_classification_loss:.5f}, Train Guidance Loss: {train_guidance_loss:.5f}, Train Penalization Term Loss: {train_penalization_term_loss:.5f}, Train Reward Term Loss: {train_reward_term_loss:.5f} | \"\n",
    "                        f\"Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.5f}\"\n",
    "                        f\"Val classification Loss: {val_classification_loss:.5f}, Val Guidance Loss: {val_guidance_loss:.5f}, Val Penalization Term Loss: {val_penalization_term_loss:.5f}, Val Reward Term Loss: {val_reward_term_loss:.5f} | \")\n",
    "                \n",
    "\n",
    "                # Early stopping check\n",
    "                # For smci_pmci_balanced, we monitor validation loss for early stopping (balanced dataset)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    # Save the model state with the lowest validation loss\n",
    "                    best_model_state = model.state_dict()\n",
    "                    best_epoch = epoch + 1\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                \n",
    "                if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    print(f\"\\nEarly stopping triggered. Restoring best model weights from epoch {best_epoch}.\\n\")\n",
    "                    break\n",
    "                \n",
    "                if USE_MODEL == \"purevit\":\n",
    "                    if train_loss < 0.15:\n",
    "                        print(f\" We stop training here because overfitting is already happening.\")\n",
    "                        break\n",
    "            \n",
    "            \n",
    "            # Restore the best model state if early stopping was not triggered\n",
    "            if epochs_no_improve < EARLY_STOP_PATIENCE:\n",
    "                print(f\"Training completed for {NUM_EPOCHS} epochs. Restoring best model weights from epoch {best_epoch}.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            # Save fold history and plots\n",
    "            fold_dir = os.path.join(combo_results_dir, f\"fold_{fold_no}\")\n",
    "            os.makedirs(fold_dir, exist_ok=True)\n",
    "            history_df = pd.DataFrame(history)\n",
    "            history_csv_path = os.path.join(fold_dir, f\"history_fold_{fold_no}.csv\")\n",
    "            history_df.to_csv(history_csv_path, index=False)\n",
    "            plot_loss_curves(history, fold_dir)\n",
    "            \n",
    "            plot_guidance_losses(history, fold_dir)\n",
    "            plot_average_attention_scores(history, fold_dir)\n",
    "\n",
    "            # Evaluate the fold\n",
    "            best_epoch_index = best_epoch - 1\n",
    "            \n",
    "            val_loss_best = history['val_loss'][best_epoch_index]\n",
    "            \n",
    "            val_classification_loss_best = history['val_classification_loss'][best_epoch_index]\n",
    "            val_guidance_loss_best = history['val_guidance_loss'][best_epoch_index]\n",
    "            val_penalization_term_loss_best = history['val_penalization_term_loss'][best_epoch_index]\n",
    "            val_reward_term_loss_best = history['val_reward_term_loss'][best_epoch_index]\n",
    "            \n",
    "            val_accuracy_best = history['val_accuracy'][best_epoch_index]\n",
    "            val_auc_best = history['val_auc'][best_epoch_index]\n",
    "            \n",
    "            val_sensitivity_best = history['val_sensitivity'][best_epoch_index]\n",
    "            val_specificity_best = history['val_specificity'][best_epoch_index]\n",
    "            val_bacc_best = history['val_bacc'][best_epoch_index]\n",
    "            val_f1_best = history['val_f1'][best_epoch_index]\n",
    "            val_mcc_best = history['val_mcc'][best_epoch_index]\n",
    "            \n",
    "            print(f\"    Fold {fold_no} - Best Epoch {best_epoch}, Val Loss: {val_loss_best:.4f}, Acc: {val_accuracy_best:.4f}, AUC: {val_auc_best:.4f}, \\n\"\n",
    "                f\"    Classification Loss: {val_classification_loss_best:.4f}, Guidance Loss: {val_guidance_loss_best:.4f}, Penalization Term Loss: {val_penalization_term_loss_best:.4f}, Reward Term Loss: {val_reward_term_loss_best:.4f}\\n\"\n",
    "                f\"    Sensitivity: {val_sensitivity_best:.4f}, Specificity: {val_specificity_best:.4f}, BACC: {val_bacc_best:.4f}, F1: {val_f1_best:.4f}, MCC: {val_mcc_best:.4f}\")\n",
    "\n",
    "            # Append results for this fold\n",
    "            current_combo_val_losses.append(val_loss_best)\n",
    "        \n",
    "            current_combo_val_accuracies.append(val_accuracy_best)\n",
    "            current_combo_val_aucs.append(val_auc_best)\n",
    "            current_combo_best_epoch.append(best_epoch)\n",
    "            \n",
    "            current_combo_val_sensitivities.append(val_sensitivity_best)\n",
    "            current_combo_val_specificities.append(val_specificity_best)\n",
    "            current_combo_val_baccs.append(val_bacc_best)\n",
    "            current_combo_val_f1s.append(val_f1_best)\n",
    "            current_combo_val_mccs.append(val_mcc_best)\n",
    "            \n",
    "            optimizer_name = optimizer.__class__.__name__\n",
    "            \n",
    "            end_time_fold = time.time()\n",
    "            print(f\"    Fold {fold_no} finished in {end_time_fold - start_time_fold:.2f} seconds.\")\n",
    "            fold_no += 1\n",
    "            \n",
    "            # Save the validation metrics for current fold\n",
    "            val_metrics_file_path = os.path.join(fold_dir, \"val_results.txt\")\n",
    "            with open(val_metrics_file_path, \"w\") as f:\n",
    "                f.write(\"--- Validation Metrics ---\\n\")\n",
    "                f.write(f\"Loss:        {val_loss_best:.4f}\\n\")\n",
    "                f.write(f\"Accuracy:    {val_accuracy_best:.4f}\\n\")\n",
    "                f.write(f\"Sensitivity: {val_sensitivity_best:.4f}\\n\")\n",
    "                f.write(f\"Specificity: {val_specificity_best:.4f}\\n\")\n",
    "                f.write(f\"Balanced Accuracy (BACC): {val_bacc_best:.4f}\\n\")\n",
    "                f.write(f\"F1 Score (Weighted):      {val_f1_best:.4f}\\n\")\n",
    "                f.write(f\"Matthews Corr Coef (MCC): {val_mcc_best:.4f}\\n\")\n",
    "                f.write(f\"Area Under the Curve (AUC): {val_auc_best:.4f}\\n\")\n",
    "                f.write(f\"Best Epoch: {best_epoch}\\n\")\n",
    "                f.write(f\"Classification Loss: {val_classification_loss_best:.4f}\\n\")\n",
    "                f.write(f\"Guidance Loss: {val_guidance_loss_best:.4f}\\n\")\n",
    "                f.write(f\"Penalization Term Loss: {val_penalization_term_loss_best:.4f}\\n\")\n",
    "                f.write(f\"Reward Term Loss: {val_reward_term_loss_best:.4f}\\n\")\n",
    "            \n",
    "            # ---------------- Evaluate the model on the test data ---------------- \n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            running_classification_loss = 0.0\n",
    "            running_guidance_loss = 0.0\n",
    "            running_penalization_term_loss = 0.0\n",
    "            running_reward_term_loss = 0.0\n",
    "            \n",
    "            running_avg_att_roi = 0.0\n",
    "            running_avg_att_non_roi = 0.0\n",
    "            \n",
    "            test_pred_probs = []\n",
    "            test_targets = []\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                for inputs, labels in test_data:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                    # Mixed precision\n",
    "                    with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=USE_AMP):\n",
    "                        outputs, attention_maps = model(inputs, return_attention=RETURN_ATTENTION)\n",
    "                        classification_loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        if USE_MODEL == \"hybrid\":\n",
    "                            attn_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True)\n",
    "                        elif USE_MODEL == \"purevit\":\n",
    "                            attn_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True) # determine the exact layer later   \n",
    "                        cls_attn = attn_map[:, 0, 1:] # CLS attention vector\n",
    "                        \n",
    "                        volume_att = upsample_vit_attention(cls_attn, EXPECTED_FINAL_VOLUME_SHAPE, (6, 7, 6)) # Get voxel-wise CLS attention map\n",
    "                        \n",
    "                        volume_att = volume_att.flatten(start_dim=1) # first dimension is the batch size, which is 1\n",
    "                        volume_att = F.softmax(volume_att / T, dim=-1)  # Apply softmax to the CLS attention vector\n",
    "                        volume_att = volume_att.reshape(len(inputs), EXPECTED_FINAL_VOLUME_SHAPE[0], EXPECTED_FINAL_VOLUME_SHAPE[1], EXPECTED_FINAL_VOLUME_SHAPE[2])  # Reshape back to the original volume shape\n",
    "\n",
    "                        penalization_term_loss = torch.mean(torch.sum(torch.multiply(torch.log(1-volume_att+1e-10), 1-mask_volume), dim=(1,2,3))) # penalization term of the guidance loss across the batch\n",
    "                        reward_term_loss = torch.mean(torch.sum(torch.multiply(torch.log(volume_att+1e-10), mask_volume), dim=(1,2,3))) # reward term of the guidance loss across the batch\n",
    "                        guidance_loss = - lambda_factor * (penalization_term_loss + reward_term_loss)\n",
    "                    \n",
    "                        loss = classification_loss + guidance_loss\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    running_classification_loss += classification_loss.item() * inputs.size(0)\n",
    "                    running_guidance_loss += guidance_loss.item() * inputs.size(0)\n",
    "                    running_penalization_term_loss += penalization_term_loss.item() * inputs.size(0)\n",
    "                    running_reward_term_loss += reward_term_loss.item() * inputs.size(0)\n",
    "                \n",
    "                    prob_outputs = torch.sigmoid(outputs)\n",
    "                    \n",
    "                    test_pred_probs.extend(prob_outputs.cpu().numpy())\n",
    "                    test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "            test_targets = np.array(test_targets).flatten()\n",
    "            test_pred_probs = np.array(test_pred_probs).flatten()\n",
    "\n",
    "            # Calculate test loss and accuracy\n",
    "            test_loss = running_loss / len(test_data.dataset)\n",
    "            \n",
    "            test_classification_loss = running_classification_loss / len(test_data.dataset)\n",
    "            test_guidance_loss = running_guidance_loss / len(test_data.dataset)\n",
    "            test_penalization_term_loss = running_penalization_term_loss / len(test_data.dataset)\n",
    "            test_reward_term_loss = running_reward_term_loss / len(test_data.dataset)\n",
    "            \n",
    "            test_acc = accuracy_score(test_targets, test_pred_probs >= 0.5)\n",
    "\n",
    "            y_true = test_targets.astype(int)\n",
    "            y_pred = (test_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "            # Calculate metrics\n",
    "            test_sensitivity = recall_score(y_true, y_pred, pos_label=1)\n",
    "            test_specificity = recall_score(y_true, y_pred, pos_label=0)\n",
    "            test_bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "            test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            test_mcc = matthews_corrcoef(y_true, y_pred)\n",
    "            test_auc_score = roc_auc_score(y_true, y_pred)\n",
    "            \n",
    "            print(f\"    Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, AUC: {test_auc_score:.4f}, \\n\"\n",
    "                f\"   Classification Loss: {test_classification_loss:.4f}, Guidance Loss: {test_guidance_loss:.4f}, Penalization Term Loss: {test_penalization_term_loss:.4f}, Reward Term Loss: {test_reward_term_loss:.4f}\\n\"\n",
    "                f\"    Sensitivity: {test_sensitivity:.4f}, Specificity: {test_specificity:.4f}, BACC: {test_bacc:.4f}, F1: {test_f1:.4f}, MCC: {test_mcc:.4f}\")\n",
    "            \n",
    "            classes = DATASET.split(\"_\")\n",
    "            class0, class1 = classes[0], classes[1]\n",
    "            target_names = [class0, class1]\n",
    "\n",
    "            # Save the confusion matrix plot\n",
    "            make_confusion_matrix(y_true=y_true,\n",
    "                                y_pred=y_pred,\n",
    "                                classes=target_names,\n",
    "                                figsize=(8, 8),\n",
    "                                text_size=15,\n",
    "                                save_dir=fold_dir)\n",
    "\n",
    "            # Save the test loss and accuracy and the evaluation metrics\n",
    "            result_file_path = os.path.join(fold_dir, \"test_results.txt\")\n",
    "            with open(result_file_path, \"w\") as f:\n",
    "                f.write(f\"[{test_loss}, {test_acc}]\\n\\n\")\n",
    "                f.write(\"--- Evaluation Metrics on Test Set ---\\n\")\n",
    "                f.write(f\"Loss:        {test_loss:.4f}\\n\")\n",
    "                f.write(f\"Accuracy:    {test_acc:.4f}\\n\")\n",
    "                f.write(f\"Sensitivity: {test_sensitivity:.4f}\\n\")\n",
    "                f.write(f\"Specificity: {test_specificity:.4f}\\n\")\n",
    "                f.write(f\"Balanced Accuracy (BACC): {test_bacc:.4f}\\n\")\n",
    "                f.write(f\"F1 Score (Weighted):      {test_f1:.4f}\\n\")\n",
    "                f.write(f\"Matthews Corr Coef (MCC): {test_mcc:.4f}\\n\")\n",
    "                f.write(f\"Area Under the Curve (AUC): {test_auc_score:.4f}\\n\")\n",
    "                f.write(f\"Classification Loss: {test_classification_loss:.4f}\\n\")\n",
    "                f.write(f\"Guidance Loss: {test_guidance_loss:.4f}\\n\")\n",
    "                f.write(f\"Penalization Term Loss: {test_penalization_term_loss:.4f}\\n\")\n",
    "                f.write(f\"Reward Term Loss: {test_reward_term_loss:.4f}\\n\")\n",
    "                \n",
    "            test_config = {\n",
    "                \"vit_config\": vit_config,\n",
    "                \"num_epochs\": int(best_epoch),\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"optimizer_params\": {\n",
    "                    \"lr\": float(learning_rate),\n",
    "                    \"weight_decay\": float(weight_decay)\n",
    "                },\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"seed\": seed,\n",
    "                \"dataset\": DATASET\n",
    "            }\n",
    "            params_file_path = os.path.join(fold_dir, \"test_config.json\")\n",
    "            with open(params_file_path, \"w\") as f:\n",
    "                json.dump(test_config, f, indent=4)\n",
    "                    \n",
    "            # Append results for this fold\n",
    "            current_combo_test_losses.append(test_loss)\n",
    "            \n",
    "            current_combo_test_accuracies.append(test_acc)\n",
    "            current_combo_test_aucs.append(test_auc_score)\n",
    "            \n",
    "            current_combo_test_sensitivities.append(test_sensitivity)\n",
    "            current_combo_test_specificities.append(test_specificity)\n",
    "            current_combo_test_baccs.append(test_bacc)\n",
    "            current_combo_test_f1s.append(test_f1)\n",
    "            current_combo_test_mccs.append(test_mcc)\n",
    "            \n",
    "            # Free resources\n",
    "            #del model, optimizer, criterion, fold_train_data, fold_val_data, history, history_df, best_model_state\n",
    "            #if device.type == 'cuda':\n",
    "                #torch.cuda.empty_cache() \n",
    "            #gc.collect() \n",
    "\n",
    "        end_time_combination = time.time()\n",
    "        print(f\"Combination finished in {end_time_combination - start_time_combination:.2f} seconds.\")\n",
    "\n",
    "        # Aggregate results across folds\n",
    "        avg_loss = np.mean(current_combo_val_losses)\n",
    "        std_loss = np.std(current_combo_val_losses)\n",
    "        \n",
    "        avg_acc = np.mean(current_combo_val_accuracies)\n",
    "        std_acc = np.std(current_combo_val_accuracies)\n",
    "        avg_auc = np.mean(current_combo_val_aucs)\n",
    "        std_auc = np.std(current_combo_val_aucs)\n",
    "        \n",
    "        avg_sensitivity = np.mean(current_combo_val_sensitivities)\n",
    "        avg_specificity = np.mean(current_combo_val_specificities)\n",
    "        avg_bacc = np.mean(current_combo_val_baccs)\n",
    "        std_bacc = np.std(current_combo_val_baccs)\n",
    "        avg_f1 = np.mean(current_combo_val_f1s)\n",
    "        std_f1 = np.std(current_combo_val_f1s)\n",
    "        avg_mcc = np.mean(current_combo_val_mccs)\n",
    "\n",
    "        combo_average_results = {\n",
    "            'avg_val_loss': avg_loss,\n",
    "            'std_val_loss': std_loss,\n",
    "            'avg_val_accuracy': avg_acc,\n",
    "            'std_val_accuracy': std_acc,\n",
    "            'avg_val_auc': avg_auc,\n",
    "            'std_val_auc': std_auc,\n",
    "            'individual_losses': [round(loss, 4) for loss in current_combo_val_losses],\n",
    "            'individual_accuracies': [round(acc, 4) for acc in current_combo_val_accuracies],\n",
    "            'individual_aucs': [round(auc, 4) for auc in current_combo_val_aucs],\n",
    "            'best_epoch_per_fold': list(current_combo_best_epoch),\n",
    "            'avg_val_sensitivity': avg_sensitivity,\n",
    "            'avg_val_specificity': avg_specificity,\n",
    "            'avg_val_bacc': avg_bacc,\n",
    "            'std_val_bacc': std_bacc,\n",
    "            'avg_val_f1': avg_f1,\n",
    "            'std_val_f1': std_f1,\n",
    "            'avg_val_mcc': avg_mcc,\n",
    "            'individual_sensitivities': [round(sensitivity, 4) for sensitivity in current_combo_val_sensitivities],\n",
    "            'individual_specificities': [round(specificity, 4) for specificity in current_combo_val_specificities],\n",
    "            'individual_baccs': [round(bacc, 4) for bacc in current_combo_val_baccs],\n",
    "            'individual_f1s': [round(f1, 4) for f1 in current_combo_val_f1s],\n",
    "            'individual_mccs': [round(mcc, 4) for mcc in current_combo_val_mccs]\n",
    "        }\n",
    "\n",
    "        # Save results in a txt file and the parameters in a json file\n",
    "        val_results_df = pd.DataFrame(combo_average_results)\n",
    "        display_cols = list(combo_average_results.keys())\n",
    "\n",
    "        val_results_filepath = os.path.join(combo_results_dir, \"validation_results.txt\")\n",
    "        with open(val_results_filepath, \"w\") as f:\n",
    "            print(\"Validation Results Summary:\", file=f)\n",
    "            print(val_results_df[display_cols].round(6).to_string(index=False, max_colwidth=None, line_width=250), file=f)\n",
    "\n",
    "            print(f\"\\nHyperparameters: Lambda={lambda_factor}\", file=f)\n",
    "        \n",
    "        avg_test_loss = np.mean(current_combo_test_losses)\n",
    "        std_test_loss = np.std(current_combo_test_losses)\n",
    "        \n",
    "        # No need to save the other losses here\n",
    "        \n",
    "        avg_test_acc = np.mean(current_combo_test_accuracies)\n",
    "        std_test_acc = np.std(current_combo_test_accuracies)\n",
    "        avg_test_auc = np.mean(current_combo_test_aucs)\n",
    "        std_test_auc = np.std(current_combo_test_aucs)\n",
    "        \n",
    "        avg_test_sensitivity = np.mean(current_combo_test_sensitivities)\n",
    "        std_test_sensitivity = np.std(current_combo_test_sensitivities)\n",
    "        avg_test_specificity = np.mean(current_combo_test_specificities)\n",
    "        std_test_specificity = np.std(current_combo_test_specificities)\n",
    "        avg_test_bacc = np.mean(current_combo_test_baccs)\n",
    "        std_test_bacc = np.std(current_combo_test_baccs)\n",
    "        avg_test_f1 = np.mean(current_combo_test_f1s)\n",
    "        std_test_f1 = np.std(current_combo_test_f1s)\n",
    "        avg_test_mcc = np.mean(current_combo_test_mccs)\n",
    "        std_test_mcc = np.std(current_combo_test_mccs)\n",
    "        \n",
    "\n",
    "        combo_test_average_results = {\n",
    "            'avg_test_loss': avg_test_loss,\t\n",
    "            'std_test_loss': std_test_loss,\n",
    "            'avg_test_accuracy': avg_test_acc,\n",
    "            'std_test_accuracy': std_test_acc,\n",
    "            'avg_test_auc': avg_test_auc,\n",
    "            'std_test_auc': std_test_auc,\n",
    "            'individual_test_losses': [round(loss, 4) for loss in current_combo_test_losses],\n",
    "            'individual_test_accuracies': [round(acc, 4) for acc in current_combo_test_accuracies],\n",
    "            'individual_test_aucs': [round(auc, 4) for auc in current_combo_test_aucs],\n",
    "            'avg_test_sensitivity': avg_test_sensitivity,\n",
    "            'std_test_sensitivity': std_test_sensitivity,\n",
    "            'avg_test_specificity': avg_test_specificity,\n",
    "            'std_test_specificity': std_test_specificity,\n",
    "            'avg_test_bacc': avg_test_bacc,\n",
    "            'std_test_bacc': std_test_bacc,\n",
    "            'avg_test_f1': avg_test_f1,\n",
    "            'std_test_f1': std_test_f1,\n",
    "            'avg_test_mcc': avg_test_mcc,\n",
    "            'std_test_mcc': std_test_mcc,\n",
    "            'individual_test_sensitivities': [round(sensitivity, 4) for sensitivity in current_combo_test_sensitivities],\n",
    "            'individual_test_specificities': [round(specificity, 4) for specificity in current_combo_test_specificities],\n",
    "            'individual_test_baccs': [round(bacc, 4) for bacc in current_combo_test_baccs],\n",
    "            'individual_test_f1s': [round(f1, 4) for f1 in current_combo_test_f1s],\n",
    "            'individual_test_mccs': [round(mcc, 4) for mcc in current_combo_test_mccs],\n",
    "        }\n",
    "            \n",
    "        test_results_df = pd.DataFrame(combo_test_average_results)\n",
    "        display_cols = list(combo_test_average_results.keys())\n",
    "        test_results_filepath = os.path.join(combo_results_dir, \"average_test_results.txt\")\n",
    "        with open(test_results_filepath, \"w\") as f:\n",
    "            print(\"Test Results Summary:\", file=f)\n",
    "            print(test_results_df[display_cols].round(6).to_string(index=False, max_colwidth=None, line_width=250), file=f)\n",
    "\n",
    "            print(f\"\\nHyperparameters: Lambda={lambda_factor}\", file=f)\n",
    "\n",
    "        training_config = {\n",
    "            \"vit_config\": vit_config,\n",
    "            \"num_folds\": N_SPLITS,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"optimizer_params\": {\n",
    "                \"lr\": float(learning_rate),\n",
    "                \"weight_decay\": float(weight_decay)\n",
    "            },\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"seed\": seed,\n",
    "            \"dataset\": DATASET\n",
    "        }\n",
    "        params_file_path = os.path.join(combo_results_dir, \"training_config.json\")\n",
    "        with open(params_file_path, \"w\") as f:\n",
    "            json.dump(training_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff2ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "Head 1 - Min: -0.0742, Max: 0.0360\n",
      "Head 2 - Min: -0.0232, Max: 0.0410\n",
      "Head 3 - Min: -0.0478, Max: 0.0435\n",
      "Head 4 - Min: -0.0404, Max: 0.0688\n",
      "Head 5 - Min: -0.0431, Max: 0.0475\n",
      "Head 6 - Min: -0.0497, Max: 0.0176\n",
      "Head 7 - Min: -0.0679, Max: 0.0510\n",
      "Head 8 - Min: -0.0369, Max: 0.0273\n",
      "Layer Min: -0.074219, Global Max: 0.068787\n",
      "Layer 2\n",
      "Head 1 - Min: -0.1538, Max: 1.0801\n",
      "Head 2 - Min: -0.4050, Max: 0.6626\n",
      "Head 3 - Min: -0.5654, Max: 0.3979\n",
      "Head 4 - Min: -0.2395, Max: 0.6313\n",
      "Head 5 - Min: -0.2585, Max: 0.5020\n",
      "Head 6 - Min: -0.6562, Max: 0.3889\n",
      "Head 7 - Min: -0.4778, Max: 0.3652\n",
      "Head 8 - Min: -0.6055, Max: 0.4741\n",
      "Layer Min: -0.656250, Global Max: 1.080078\n",
      "Layer 3\n",
      "Head 1 - Min: -0.3967, Max: 0.6958\n",
      "Head 2 - Min: -0.1857, Max: 0.5244\n",
      "Head 3 - Min: -0.1572, Max: 0.9165\n",
      "Head 4 - Min: -0.8350, Max: 0.4333\n",
      "Head 5 - Min: 0.0842, Max: 1.1279\n",
      "Head 6 - Min: -0.4172, Max: 0.7524\n",
      "Head 7 - Min: -0.6675, Max: 0.2671\n",
      "Head 8 - Min: -0.5498, Max: 0.3687\n",
      "Layer Min: -0.834961, Global Max: 1.127930\n",
      "Layer 4\n",
      "Head 1 - Min: -0.4626, Max: 0.4397\n",
      "Head 2 - Min: -0.4102, Max: 0.4827\n",
      "Head 3 - Min: -0.3208, Max: 0.5239\n",
      "Head 4 - Min: 0.3145, Max: 1.1084\n",
      "Head 5 - Min: -0.4990, Max: 0.4373\n",
      "Head 6 - Min: -0.3657, Max: 0.3154\n",
      "Head 7 - Min: -0.8213, Max: 0.3682\n",
      "Head 8 - Min: -0.5088, Max: 0.4509\n",
      "Layer Min: -0.821289, Global Max: 1.108398\n",
      "Layer 5\n",
      "Head 1 - Min: -0.3662, Max: 0.3872\n",
      "Head 2 - Min: -0.5273, Max: 0.3164\n",
      "Head 3 - Min: -0.5298, Max: 0.5063\n",
      "Head 4 - Min: -0.1571, Max: 0.8589\n",
      "Head 5 - Min: -0.4043, Max: 0.4297\n",
      "Head 6 - Min: -0.7246, Max: 0.4233\n",
      "Head 7 - Min: -0.4209, Max: 0.5918\n",
      "Head 8 - Min: -0.4565, Max: 0.4724\n",
      "Layer Min: -0.724609, Global Max: 0.858887\n",
      "Layer 6\n",
      "Head 1 - Min: -0.9795, Max: 0.3452\n",
      "Head 2 - Min: -0.6206, Max: 0.2522\n",
      "Head 3 - Min: -0.7271, Max: 0.4451\n",
      "Head 4 - Min: -0.0253, Max: 1.0898\n",
      "Head 5 - Min: -0.4609, Max: 0.3542\n",
      "Head 6 - Min: -0.6006, Max: 0.2130\n",
      "Head 7 - Min: -0.3467, Max: 0.4749\n",
      "Head 8 - Min: -0.3271, Max: 0.5850\n",
      "Layer Min: -0.979492, Global Max: 1.089844\n",
      "Layer 7\n",
      "Head 1 - Min: -0.0387, Max: 1.0127\n",
      "Head 2 - Min: -0.1877, Max: 0.7183\n",
      "Head 3 - Min: 0.0185, Max: 0.8252\n",
      "Head 4 - Min: -0.4045, Max: 0.4197\n",
      "Head 5 - Min: -0.3616, Max: 0.4907\n",
      "Head 6 - Min: -0.5073, Max: 0.4622\n",
      "Head 7 - Min: -0.6182, Max: 0.2539\n",
      "Head 8 - Min: -0.6440, Max: 0.2578\n",
      "Layer Min: -0.644043, Global Max: 1.012695\n"
     ]
    }
   ],
   "source": [
    "# Check min and max raw attention values for each layer and head\n",
    "for j in range(1, 8):\n",
    "    print(f'Layer {j}')\n",
    "    global_min = float('inf')\n",
    "    global_max = float('-inf')\n",
    "    for i in range (1, 9):\n",
    "        attn_map = model.get_attention_map(layer=j, head=i, average_heads=False)\n",
    "        curr_min = np.min(attn_map[:, 0, 1:].detach().cpu().numpy())\n",
    "        curr_max = np.max(attn_map[:, 0, 1:].detach().cpu().numpy())\n",
    "        print(f\"Head {i} - Min: {curr_min:.4f}, Max: {curr_max:.4f}\")\n",
    "        if curr_min < global_min:\n",
    "            global_min = curr_min\n",
    "        if curr_max > global_max:\n",
    "            global_max = curr_max\n",
    "    print(f\"Layer Min: {global_min:.6f}, Global Max: {global_max:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02547fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "Average scores - Min: -0.0392, Max: 0.0246\n",
      "Layer 2\n",
      "Average scores - Min: -0.1362, Max: 0.2913\n",
      "Layer 3\n",
      "Average scores - Min: -0.0953, Max: 0.3140\n",
      "Layer 4\n",
      "Average scores - Min: -0.1594, Max: 0.1874\n",
      "Layer 5\n",
      "Average scores - Min: -0.1578, Max: 0.2581\n",
      "Layer 6\n",
      "Average scores - Min: -0.1465, Max: 0.1317\n",
      "Layer 7\n",
      "Average scores - Min: -0.0810, Max: 0.2517\n"
     ]
    }
   ],
   "source": [
    "# Check min and max average attention values for each layer\n",
    "global_min = float('inf')\n",
    "global_max = float('-inf')\n",
    "for j in range(1, 8):\n",
    "    print(f'Layer {j}')\n",
    "    attn_map = model.get_attention_map(layer=j, head=None, average_heads=True)\n",
    "    curr_min = np.min(attn_map[:, 0, 1:].detach().cpu().numpy())\n",
    "    curr_max = np.max(attn_map[:, 0, 1:].detach().cpu().numpy())\n",
    "    print(f\"Average scores - Min: {curr_min:.4f}, Max: {curr_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9f7e9679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention map shape: torch.Size([1, 252])\n",
      "Softmaxed attention map shape: torch.Size([1, 252])\n",
      "Min probability: 0.0034, Max probability: 0.0045\n"
     ]
    }
   ],
   "source": [
    "# Check probability values for CLS attention vector - confirm if they check out\n",
    "att_map = model.get_attention_map(layer=CLS_LAYER, head=None, average_heads=True) # determine the exact layer later\n",
    "print(f\"Attention map shape: {attn_map.shape}\")\n",
    "\n",
    "# softmax att_mat scores \n",
    "softmaxed_att_map = F.softmax(att_map[:, 0, 1:], dim=-1)  # Apply softmax to the CLS attention vector\n",
    "print(f\"Softmaxed attention map shape: {softmaxed_att_map.shape}\")\n",
    "# get max and min probabilities\n",
    "prob_max = torch.max(softmaxed_att_map, dim=-1).values\n",
    "prob_min = torch.min(softmaxed_att_map, dim=-1).values\n",
    "print(f\"Min probability: {prob_min.mean().item():.4f}, Max probability: {prob_max.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b290ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume attention shape: torch.Size([1, 91, 109, 91])\n",
      "Volume attention - Min: -0.1465, Max: 0.1317\n",
      "Softmaxed volume attention shape: torch.Size([1, 91, 109, 91])\n",
      "Softmaxed volume attention - Min: 0.0000008345, Max: 0.0000014305\n",
      "Sum of softmaxed volume attention: 0.99951172\n"
     ]
    }
   ],
   "source": [
    "# upsample the raw attention values to image-level\n",
    "volume_att = upsample_vit_attention(att_map[:, 0, 1:], EXPECTED_FINAL_VOLUME_SHAPE, (6, 7, 6)) # Get voxel-wise CLS attention map\n",
    "print(f\"Volume attention shape: {volume_att.shape}\")\n",
    "# print max and min of the volume attention\n",
    "print(f\"Volume attention - Min: {torch.min(volume_att).item():.4f}, Max: {torch.max(volume_att).item():.4f}\")\n",
    "\n",
    "softmaxed_volume_att = volume_att.view(1, -1) # first dimension is the batch size, which is 1\n",
    "softmaxed_volume_att = F.softmax(softmaxed_volume_att / 1, dim=-1)  # Apply softmax to the CLS attention vector\n",
    "softmaxed_volume_att = softmaxed_volume_att.view(1, EXPECTED_FINAL_VOLUME_SHAPE[0], EXPECTED_FINAL_VOLUME_SHAPE[1], EXPECTED_FINAL_VOLUME_SHAPE[2])  # Reshape back to the original volume shape\n",
    "print(f\"Softmaxed volume attention shape: {softmaxed_volume_att.shape}\")\n",
    "\n",
    "print(f\"Softmaxed volume attention - Min: {torch.min(softmaxed_volume_att).item():.10f}, Max: {torch.max(softmaxed_volume_att).item():.10f}\")\n",
    "\n",
    "# confirm if the sum is 1\n",
    "print(f\"Sum of softmaxed volume attention: {torch.sum(softmaxed_volume_att).item():.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "12fd8c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI attention values - Min: -0.1009, Max: 0.1317\n",
      "Non-ROI attention values - Min: -0.1465, Max: 0.1317\n"
     ]
    }
   ],
   "source": [
    "roi_att_values = torch.multiply(volume_att, mask_volume)\n",
    "non_roi_att_values = torch.multiply(volume_att, 1-mask_volume)\n",
    "print(f\"ROI attention values - Min: {torch.min(roi_att_values).item():.4f}, Max: {torch.max(roi_att_values).item():.4f}\")\n",
    "print(f\"Non-ROI attention values - Min: {torch.min(non_roi_att_values).item():.4f}, Max: {torch.max(non_roi_att_values).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of ROI attention values: 38485.1172\n",
      "Sum of Non-ROI attention values: -4809.6509\n",
      "Absolute sum of ROI attention values: 52950.7422\n",
      "Absolute sum of Non-ROI attention values: 38537.2383\n"
     ]
    }
   ],
   "source": [
    "# sum of the attention values in the ROI and non-ROI regions\n",
    "sum_roi_att = torch.sum(roi_att_values)\n",
    "sum_non_roi_att = torch.sum(non_roi_att_values)\n",
    "print(f\"Sum of ROI attention values: {sum_roi_att.item():.4f}\")\n",
    "print(f\"Sum of Non-ROI attention values: {sum_non_roi_att.item():.4f}\")\n",
    "\n",
    "# absolute sum of the attention values\n",
    "abs_sum_roi_att = torch.sum(torch.abs(roi_att_values))\n",
    "abs_sum_non_roi_att = torch.sum(torch.abs(non_roi_att_values))\n",
    "print(f\"Absolute sum of ROI attention values: {abs_sum_roi_att.item():.4f}\")\n",
    "print(f\"Absolute sum of Non-ROI attention values: {abs_sum_non_roi_att.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3620011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create histograms of the attention values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(roi_att_values.view(-1).detach().cpu().numpy(), bins=100, alpha=0.7, color='blue', label='ROI Attention Values')\n",
    "plt.title('Histogram of ROI Attention Values')\n",
    "plt.xlabel('Attention Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(non_roi_att_values.view(-1).detach().cpu().numpy(), bins=100, alpha=0.7, color='red', label='Non-ROI Attention Values')\n",
    "plt.title('Histogram of Non-ROI Attention Values')\n",
    "plt.xlabel('Attention Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "043afa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlations: [0.39817178, 0.31769574, 0.15822315, 0.16713826, -0.07831353, 0.33660194, -0.23532745]\n",
      "Spearman Correlations: [0.5438213203309505, 0.4553772138304637, 0.20101457566598194, 0.2373431312251028, 0.0376345366721395, 0.27428845187072354, -0.28026444617298796]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr, spearmanr # For Pearson and Spearman correlation\n",
    "\n",
    "roi_scores = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017463235184550285, 0.005055147223174572, 0.0, 0.02366727963089943, 0.0, 0.0, 0.0185546875, 0.0029296875, 0.0029296875, 0.052978515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013786765048280358, 0.0, 0.0, 0.01171875, 0.0004595588252414018, 0.0986328125, 0.262451171875, 0.01123046875, 0.0068359375, 0.251220703125, 0.12158203125, 0.13947610557079315, 0.5436580777168274, 0.2709099352359772, 0.2203584611415863, 0.5009191036224365, 0.23322610557079315, 0.019775390625, 0.21484375, 0.09423828125, 0.10498046875, 0.180419921875, 0.039794921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10202205926179886, 0.0002297794126207009, 0.0002297794126207009, 0.1812959611415863, 0.008961397223174572, 0.230712890625, 0.572021484375, 0.20654296875, 0.20458984375, 0.565185546875, 0.2958984375, 0.23782169818878174, 0.4368106722831726, 0.06387867778539658, 0.06364889442920685, 0.4188878536224365, 0.3375459611415863, 0.01318359375, 0.05908203125, 0.00244140625, 0.0029296875, 0.033203125, 0.0302734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06135110184550285, 0.14085477590560913, 0.16107536852359772, 0.03791360184550285, 0.0013786765048280358, 0.0341796875, 0.40234375, 0.352294921875, 0.40380859375, 0.265625, 0.06005859375, 0.010799632407724857, 0.10133272409439087, 0.004365808796137571, 0.0011488971067592502, 0.016773896291851997, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009420955553650856, 0.11741727590560913, 0.10822610557079315, 0.004136029630899429, 0.0, 0.0, 0.195556640625, 0.3671875, 0.29296875, 0.178955078125, 0.004638671875, 0.0, 0.01585477963089943, 0.0, 0.0, 0.014246323145925999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0185546875, 0.004150390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "roi_scores = np.array(roi_scores, dtype=np.float32)\n",
    "\n",
    "# ---------- Token-level correlation analysis ----------\n",
    "\n",
    "pearson_correlations = []\n",
    "spearmanr_correlations = []\n",
    "\n",
    "for i in range (1, 8):\n",
    "    attn_map = model.get_attention_map(layer=i, head=None, average_heads=True)\n",
    "    attn_map = attn_map[:, 0, 1:]  # CLS attention vector\n",
    "    softmaxed_attn_map = torch.softmax(attn_map, dim=-1).squeeze()  # Apply softmax to the CLS attention vector\n",
    "    pearson_corr = pearsonr(softmaxed_attn_map.detach().cpu().numpy(), roi_scores)[0]\n",
    "    pearson_correlations.append(pearson_corr)\n",
    "    spearmanr_corr = spearmanr(softmaxed_attn_map.detach().cpu().numpy(), roi_scores)[0]\n",
    "    spearmanr_correlations.append(spearmanr_corr)\n",
    "\n",
    "print('Pearson Correlations:', pearson_correlations)\n",
    "print('Spearman Correlations:', spearmanr_correlations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb636a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
